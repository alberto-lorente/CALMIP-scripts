{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1e57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import sys \n",
    "import warnings \n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import whoami, HfFolder\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb35d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") \n",
    "# log_hf()\n",
    "load_dotenv(\"env_vars.env\")\n",
    "\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41adceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Models/Qwen2.5-0.5B\"\n",
    "batch_size = 8\n",
    "n_epochs = 2\n",
    "lr = 1e-5\n",
    "lora_r = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfc8d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Preapring the Data\n"
     ]
    }
   ],
   "source": [
    "########################################################## DATA WORK\n",
    "print(\"_________________________________\")\n",
    "print(\"Preapring the Data\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"df_from_exp_to_imp.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(formatted_prompt, label=True, add_generation_prompt=False, context_length=512, output_messages_list=False):\n",
    "\n",
    "    messages = format_message(formatted_prompt, label)\n",
    "    tokenized = tokenizer.apply_chat_template(messages, \n",
    "                                                tokenize=True, \n",
    "                                                add_generation_prompt=add_generation_prompt,\n",
    "                                                padding=\"max_length\",\n",
    "                                                truncation=True,\n",
    "                                                max_length=context_length,\n",
    "                                                return_dict=True,\n",
    "                                                return_tensors=\"pt\")\n",
    "    if output_messages_list:\n",
    "        return tokenized, messages\n",
    "\n",
    "    return tokenized\n",
    "    \n",
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "t_1 = []\n",
    "t_2 = []\n",
    "\n",
    "for split in df[\"split\"].unique():\n",
    "\n",
    "    split_df_1 = df[(df[\"split\"] == split) & (df[\"time\"] == 1)]\n",
    "    split_df_2 = df[(df[\"split\"] == split) & (df[\"time\"] == 2)]\n",
    "\n",
    "    hf_split_1 = Dataset.from_pandas(split_df_1)\n",
    "    hf_split_2 = Dataset.from_pandas(split_df_2)\n",
    "    \n",
    "    t_1.append(hf_split_1)\n",
    "    t_2.append(hf_split_2)\n",
    "\n",
    "hf_time_1 = DatasetDict({t_1[0][\"split\"][0]: t_1[0], \n",
    "                        t_1[1][\"split\"][0]: t_1[1],\n",
    "                        t_1[2][\"split\"][0]: t_1[2]})\n",
    "\n",
    "hf_time_2 = DatasetDict({t_2[0][\"split\"][0]: t_2[0], \n",
    "                        t_2[1][\"split\"][0]: t_2[1],\n",
    "                        t_2[2][\"split\"][0]: t_2[2]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d238e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 1310.51 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:04<00:00, 1411.97 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 1436.12 examples/s]\n",
      "Map: 100%|██████████| 1019/1019 [00:00<00:00, 1447.63 examples/s]\n",
      "Map: 100%|██████████| 8155/8155 [00:05<00:00, 1407.28 examples/s]\n",
      "Map: 100%|██████████| 1020/1020 [00:00<00:00, 1418.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "########################################################## TOKENIZER WORK\n",
    "\n",
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "hf_time_2 = hf_time_2.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "\n",
    "hf_time_1.set_format(\"torch\")\n",
    "hf_time_2.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \"target\", \"implied_statement\", \"split\", \"time\", \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "for split in hf_time_1:\n",
    "    if split != \"test\":\n",
    "        hf_time_1[split] = hf_time_1[split].remove_columns(cols_to_remove)\n",
    "        hf_time_2[split] = hf_time_2[split].remove_columns(cols_to_remove)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "hf_time_1_train_loader = DataLoader(hf_time_1[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_validation_loader = DataLoader(hf_time_1[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_test_loader = DataLoader(hf_time_1[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "hf_time_2_train_loader = DataLoader(hf_time_2[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_validation_loader = DataLoader(hf_time_2[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_test_loader = DataLoader(hf_time_2[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "# ### So far, created the prompt, did the messages with the prompt and answer in place. Applied to chat template and tokenized \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c0c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Loading the model and model config\n",
      "Model Size before LoRA 315119488\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "\n",
      "Model After LoRA\n",
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "########################3#################### MODEL WORK\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading the model and model config\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\",\n",
    "                                            quantization_config=bnb_config\n",
    "                                            )\n",
    "\n",
    "# to deal with the fact that we dont make the first token prediction??\n",
    "\n",
    "\n",
    "model_size_before = sum(t.numel() for t in model.parameters())\n",
    "print(\"Model Size before LoRA\", model_size_before)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print(\"Model After LoRA\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Training the model\")\n",
    "print()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.train()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(hf_time_1_train_loader):\n",
    "        if i > 0:\n",
    "            continue\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"\\tBatch: \", i)\n",
    "        # print(batch)\n",
    "        batch.to(device)\n",
    "        # print(batch.keys())\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        output = model(**batch)\n",
    "        logits = output.logits\n",
    "        loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "        print(batch.keys())\n",
    "        print(loss.detach().item())\n",
    "        print(output.logits.shape)\n",
    "        print(output.probas)\n",
    "\n",
    "        if i > 3:\n",
    "            continue\n",
    "\n",
    "    epoch_loss = sum(losses)/len(hf_time_1_train_loader)\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        for i, batch in enumerate(hf_time_1_validation_loader):\n",
    "            if i > 0:\n",
    "                continue\n",
    "            batch.to(device)\n",
    "            batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "            logits = output.logits\n",
    "            val_loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "            val_losses.append(val_loss.detach().item())\n",
    "\n",
    "        val_loss_epoch = sum(val_losses)/len(hf_time_1_validation_loader)\n",
    "        print(f\"Epoch {epoch} Validation Loss: {val_loss_epoch}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "for i, test_batch in enumerate(hf_time_1[\"test\"]):\n",
    "\n",
    "    if i > 0:\n",
    "        break\n",
    "    \n",
    "    text = test_batch[\"formatted_prompt\"]\n",
    "    tokenized_chat_template, messages_list = preprocess_and_tokenize(text, label=False, add_generation_prompt=True, output_messages_list=True)\n",
    "    output = model.generate(**tokenized_chat_template.to(device))\n",
    "    pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(text)\n",
    "    print(tokenized_chat_template)\n",
    "    print(output)\n",
    "    print(pred)\n",
    "\n",
    "print(\"CHECKING GENERATION\")\n",
    "print(messages_list)\n",
    "\n",
    "print(tokenized_chat_template)\n",
    "print(output)\n",
    "\n",
    "print(type(output))\n",
    "print(output.shape)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Saving the model and Tokenizer\")\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "model.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "tokenizer.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "\n",
    "print(\"RUN SUCCESSFULLY\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

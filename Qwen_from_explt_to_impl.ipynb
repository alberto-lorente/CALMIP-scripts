{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1e57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import sys \n",
    "import warnings \n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import whoami, HfFolder\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76173a",
   "metadata": {},
   "source": [
    "## Data Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a33267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss, Softmax\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b513bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Models/Qwen2.5-0.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4478bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hf():\n",
    "    \n",
    "    load_dotenv(\"env_vars.env\")\n",
    "    hf_token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    HfFolder.save_token(hf_token)\n",
    "    return print(whoami()[\"name\"])\n",
    "\n",
    "def setup():\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return local_rank\n",
    "\n",
    "def save_results_csv(df, experiment_name, model_id, cl_technique, result_type=\"specific\"):\n",
    "\n",
    "    cl_technique_clean = cl_technique.replace(\" + \", \"__\")\n",
    "    id_ = model_id.replace(\"/\", \"-\") + \"_\" + cl_technique_clean + \"_\" + str(date.today())\n",
    "    id_clean = experiment_name + id_.replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\",\"-\") + result_type + \".csv\"\n",
    "    df.to_csv(id_clean, index=False)\n",
    "    return print(\"Saved in path: \", id_clean)\n",
    "\n",
    "def clean_cl_name(cl_name):\n",
    "\n",
    "    regex = r'<(?:[\\w\\.]+)?\\.([\\w]+) object at'\n",
    "    matches =   re.findall(regex, cl_name)\n",
    "    clean_string = \" + \".join(matches)\n",
    "    return clean_string\n",
    "\n",
    "def clean_metric_name(metric_name):\n",
    "\n",
    "    reg = r\"\\s([a-z_1]+)\\s\"\n",
    "    match_ = re.search(reg, metric_name)\n",
    "    clean_str = match_.group().strip()\n",
    "\n",
    "    return clean_str\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n",
    "\n",
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def get_probability_distribution(logits):\n",
    "    probability_dist = Softmax(dim=-1)(logits)\n",
    "    return probability_dist\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100) # ignore the left pad tokens\n",
    "def loss_f(logits, labels):\n",
    "\n",
    "    flat_logits = logits.view(-1, logits.size(-1))\n",
    "    flat_labels = labels.view(-1)\n",
    "\n",
    "    loss = loss_fn(flat_logits, flat_labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def translate_prediction_to_label(text):\n",
    "    if \"NOT HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    elif \"NOT_HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT_HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else: \n",
    "            return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ddb58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id + \"/Tokenizer\")\n",
    "if tokenizer.pad_token is None and \"Llama\" in model_id: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "if tokenizer.pad_token is None and \"Qwen\" in model_id: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = open(model_id + \"/Tokenizer/chat_template.jinja\").read()\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=512):\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # for the cross entropy loss\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4266220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f6ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return local_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8f15e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"df_from_exp_to_imp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f3dbb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:750: UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     local_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m world_size \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------Preparing the Data-----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_________________________________\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:2330\u001b[0m, in \u001b[0;36mget_world_size\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[0;32m   2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1096\u001b[0m, in \u001b[0;36m_get_group_size\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a given group's world size.\"\"\"\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1302\u001b[0m, in \u001b[0;36m_get_default_group\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[1;32m-> 1302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember\u001b[38;5;241m.\u001b[39mWORLD)\n",
      "\u001b[1;31mValueError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    local_rank = setup()\n",
    "except:\n",
    "    local_rank = 1\n",
    "device = torch.device(f\"cuda:{local_rank}\")\n",
    "world_size = dist.get_world_size()\n",
    "\n",
    "\n",
    "print(\"----------Preparing the Data-----------------\")\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading and filtering the Data\")\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "\n",
    "times_array = list(df[\"time\"].unique())\n",
    "datasets = []\n",
    "dataset_names = list(df[\"task\"].unique())\n",
    "\n",
    "for time in times_array:\n",
    "\n",
    "    time_ds = []\n",
    "    for split in df[\"split\"].unique():\n",
    "\n",
    "        split_df = df[(df[\"split\"] == split) & (df[\"time\"] == time)]\n",
    "        hf_split = Dataset.from_pandas(split_df)\n",
    "        time_ds.append(hf_split)\n",
    "    datasets.append(time_ds)\n",
    "\n",
    "hf_datasets = []\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "\n",
    "    hf_ds = DatasetDict({dataset[0][\"split\"][0]: dataset[0], \n",
    "                        dataset[1][\"split\"][0]: dataset[1],\n",
    "                        dataset[2][\"split\"][0]: dataset[2]})\n",
    "    hf_ds_name = dataset_names[i]\n",
    "    hf_datasets.append({hf_ds_name: hf_ds})\n",
    "\n",
    "hf_datasets = [\n",
    "    {task_name: hf_time.map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items() \n",
    "]\n",
    "\n",
    "n_samples_per_ds = [\n",
    "    len(hf_time[\"train\"])\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items() \n",
    "]\n",
    "\n",
    "for ds in hf_datasets:\n",
    "    for hf_data in ds.values():\n",
    "        hf_data.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \n",
    "                \"target\", \"implied_statement\", \"split\", \"time\", \"task\",\n",
    "                \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "hf_datasets = [\n",
    "    {task_name: {split: hf_time[split].remove_columns(cols_to_remove)}}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items()\n",
    "    for split in hf_time \n",
    "    if split != \"test\"]\n",
    "\n",
    "print(\"hf_datasets before data collator:\")\n",
    "print(hf_datasets)\n",
    "print()\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# distributed_samplers = [\n",
    "#     {task_name: {split: DistributedSampler(hf_time[split], num_replicas=world_size, rank=local_rank, shuffle=False)}}\n",
    "#     for hf_data in hf_datasets\n",
    "#     for task_name, hf_time in hf_data.items()\n",
    "#     for split in hf_time \n",
    "#     if split != \"test\"\n",
    "# ]\n",
    "\n",
    "distributed_samplers = []\n",
    "for ds in hf_datasets:\n",
    "    ds_dict = {}\n",
    "    print(\"ds:\")\n",
    "    print(ds)\n",
    "    for task_name, hf_data in ds.items():\n",
    "        print(\"task_name:\")\n",
    "        print(task_name)\n",
    "        print(\"hf_data:\")\n",
    "        print(hf_data)\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            print(\"split:\")\n",
    "            print(split)\n",
    "            if split != \"test\":\n",
    "                distr_sampler = DistributedSampler(hf_data[split], num_replicas=world_size, rank=local_rank, shuffle=False)\n",
    "                ds_dict[task_name][split] = distr_sampler\n",
    "        dsitr_samplers.append(ds_dict)\n",
    "\n",
    "data_loaders = []\n",
    "for i, distr_sampler in enumerate(distributed_samplers):\n",
    "    ds_name = list(distr_sampler.keys())[0]\n",
    "    ds_dict = {}\n",
    "    ds_dict[ds_name] = {}\n",
    "    for split, distributed_sampler in distr_sampler[ds_name].items():\n",
    "        data_loader = DataLoader(hf_datasets[i][ds_name][split], collate_fn=data_collator, batch_size=batch_size, sampler=distributed_sampler)\n",
    "        ds_dict[ds_name][split] = data_loader\n",
    "    data_loaders.append(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd59cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "distributed_samplers = [\n",
    "    {task_name: {split: DistributedSampler(hf_time[split], num_replicas=world_size, rank=local_rank, shuffle=False)}}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items()\n",
    "    for split in hf_time \n",
    "    if split != \"test\"\n",
    "]\n",
    "\n",
    "data_loaders = []\n",
    "for i, distr_sampler in enumerate(distributed_samplers):\n",
    "    ds_name = list(distr_sampler.keys())[0]\n",
    "    ds_dict = {}\n",
    "    ds_dict[ds_name] = {}\n",
    "    for split, distributed_sampler in distr_sampler[ds_name].items():\n",
    "        data_loader = DataLoader(hf_datasets[i][ds_name][split], collate_fn=data_collator, batch_size=batch_size, sampler=distributed_sampler)\n",
    "        ds_dict[ds_name][split] = data_loader\n",
    "    data_loaders.append(ds_dict)\n",
    "\n",
    "# data loader = []\n",
    "# each item in the list is a dictionary of {<dataset_name>: {<split>: <dataloade>}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc206b0",
   "metadata": {},
   "source": [
    "## Model Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97446dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "549fe56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Models/Llama-3.2-1B-Instruct/Model\"\n",
    "tokenizer_id = \"Models/Llama-3.2-1B-Instruct/Tokenizer\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# model.save_pretrained(\"Models/Llama-3.2-1B-Instruct/Model\")\n",
    "# tokenizer.save_pretrained(\"Models/Llama-3.2-1B-Instruct/Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb35d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") \n",
    "# log_hf()\n",
    "load_dotenv(\"env_vars.env\")\n",
    "\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41adceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_epochs = 2\n",
    "lr = 1e-5\n",
    "lora_r = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546388f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Preapring the Data\n"
     ]
    }
   ],
   "source": [
    "########################################################## DATA WORK\n",
    "print(\"_________________________________\")\n",
    "print(\"Preapring the Data\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"df_from_exp_to_imp.csv\")\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be09a2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66202d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b3cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44abdf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = '<|finetune_right_pad_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42d07937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|finetune_right_pad_id|>\n",
      "list[0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(type(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b884688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.pad_token, add_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ab66f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfafd5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "print(tokens.shape)\n",
    "second = tokens.fill_(-100)\n",
    "tok = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# torch_tensor = torch.tensor(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e14b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100, 128000,   9906,\n",
       "             11,   1268,    527,    499,     30]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both = torch.cat((tokens,tok), dim=1)\n",
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f9cd57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "517b1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(both)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6e711b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|finetune_right_pad_id|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9104a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c33bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33bf640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=312):\n",
    "\n",
    "    # if type(label) != list:\n",
    "    #     label = [label]\n",
    "    # if type(clean_post) != list:\n",
    "    #     clean_post = [clean_post]\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    # pp(prompt_plus_messages)\n",
    "    # pp(label)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    # print(messages)\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    # print(chat_template)\n",
    "\n",
    "    # why is the chat template putting a new line at the end of the end of sequence\n",
    "    # pp(chat_template)\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "    # print(input_ids_tokenized)\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # acc to llama docs\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    print(\"FILLING PAD WITH\")\n",
    "    print(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    eos_n_tensor.fill_(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a54d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(logits, labels):\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfc8d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "t_1 = []\n",
    "t_2 = []\n",
    "\n",
    "for split in df[\"split\"].unique():\n",
    "\n",
    "    split_df_1 = df[(df[\"split\"] == split) & (df[\"time\"] == 1)]\n",
    "    split_df_2 = df[(df[\"split\"] == split) & (df[\"time\"] == 2)]\n",
    "\n",
    "    hf_split_1 = Dataset.from_pandas(split_df_1)\n",
    "    hf_split_2 = Dataset.from_pandas(split_df_2)\n",
    "    \n",
    "    t_1.append(hf_split_1)\n",
    "    t_2.append(hf_split_2)\n",
    "\n",
    "hf_time_1 = DatasetDict({t_1[0][\"split\"][0]: t_1[0], \n",
    "                        t_1[1][\"split\"][0]: t_1[1],\n",
    "                        t_1[2][\"split\"][0]: t_1[2]})\n",
    "\n",
    "hf_time_2 = DatasetDict({t_2[0][\"split\"][0]: t_2[0], \n",
    "                        t_2[1][\"split\"][0]: t_2[1],\n",
    "                        t_2[2][\"split\"][0]: t_2[2]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1fb5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = hf_time_1[\"train\"][0]\n",
    "input_model = preprocess_and_tokenize(ex[\"clean_post\"], ex[\"label\"], base_prompt=base_prompt, max_length=312)\n",
    "input_model[\"labels\"]\n",
    "input_model[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e04f61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = model(**input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2cd14464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3625, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e16c4d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 311, 128256])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a638b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 311])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_model[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01330643",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output_model.logits\n",
    "labels = input_model[\"labels\"]\n",
    "\n",
    "flat_logits = logits.view(-1, logits.size(-1))\n",
    "flat_labels = labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "523653e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311, 128256])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7eaaf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a2af1681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9375, dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss(ignore_index=-100)  \n",
    "loss = loss_fn(flat_logits, flat_labels)          \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c78dd8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7edf5",
   "metadata": {},
   "source": [
    "Checking that the -100 is not being computed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d32272c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (flat_labels != -100)\n",
    "mask.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1b55fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_logits = flat_logits[mask]   \n",
    "valid_labels = flat_labels[mask]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12140e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9375, dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_loss = loss_fn(valid_logits, valid_labels)\n",
    "manual_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(logits, labels):\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d238e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 1300.52 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:04<00:00, 1307.97 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 1364.92 examples/s]\n",
      "Map: 100%|██████████| 1019/1019 [00:00<00:00, 1268.26 examples/s]\n",
      "Map: 100%|██████████| 8155/8155 [00:06<00:00, 1311.17 examples/s]\n",
      "Map: 100%|██████████| 1020/1020 [00:00<00:00, 1156.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "########################################################## TOKENIZER WORK\n",
    "\n",
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "hf_time_2 = hf_time_2.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebaec1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT HATEFUL'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_time_1[\"train\"][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79750cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67cb97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7482fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(151643)\n",
    "end_of_text_token = 151643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5891c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_encoding = tokenizer.encode(\"HATEFUL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66a221fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hate_encoding = tokenizer.encode(\"NOT HATEFUL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddcef0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'formatted_prompt', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = hf_time_1[\"train\"][0]\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "135abd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_prompt = example[\"input_ids\"][0].index(end_of_text_token)\n",
    "end_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24b8e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14065, 472, 2336, 49636, 151643]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoding = tokenizer.encode(example[\"label\"] + tokenizer.eos_token)\n",
    "label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6226d8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(14065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d17091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_answer = end_prompt-(len(label_encoding)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0][: start_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81880492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT HATEFUL<|im_end|>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0][start_answer : end_prompt - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7a9e8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'formatted_prompt', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01060ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "10fdb98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0739894",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=312):\n",
    "\n",
    "    # if type(label) != list:\n",
    "    #     label = [label]\n",
    "    # if type(clean_post) != list:\n",
    "    #     clean_post = [clean_post]\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    # pp(prompt_plus_messages)\n",
    "    # pp(label)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    # print(messages)\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    # print(chat_template)\n",
    "\n",
    "    # why is the chat template putting a new line at the end of the end of sequence\n",
    "    # pp(chat_template)\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "    # print(input_ids_tokenized)\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # acc to llama docs\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.eos_token_id)\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "171650b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 821.43 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:07<00:00, 818.96 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:01<00:00, 622.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_time_1.set_format(\"torch\")\n",
    "hf_time_2.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \"target\", \"implied_statement\", \"split\", \"time\", \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "for split in hf_time_1:\n",
    "    if split != \"test\":\n",
    "        hf_time_1[split] = hf_time_1[split].remove_columns(cols_to_remove)\n",
    "        hf_time_2[split] = hf_time_2[split].remove_columns(cols_to_remove)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "hf_time_1_train_loader = DataLoader(hf_time_1[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_validation_loader = DataLoader(hf_time_1[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_test_loader = DataLoader(hf_time_1[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "hf_time_2_train_loader = DataLoader(hf_time_2[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_validation_loader = DataLoader(hf_time_2[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_test_loader = DataLoader(hf_time_2[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "# ### So far, created the prompt, did the messages with the prompt and answer in place. Applied to chat template and tokenized \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c0c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Loading the model and model config\n",
      "Model Size before LoRA 315119488\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "\n",
      "Model After LoRA\n",
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "########################3#################### MODEL WORK\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading the model and model config\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\",\n",
    "                                            quantization_config=bnb_config\n",
    "                                            )\n",
    "\n",
    "# to deal with the fact that we dont make the first token prediction??\n",
    "\n",
    "\n",
    "model_size_before = sum(t.numel() for t in model.parameters())\n",
    "print(\"Model Size before LoRA\", model_size_before)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print(\"Model After LoRA\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Training the model\")\n",
    "print()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.train()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(hf_time_1_train_loader):\n",
    "        if i > 0:\n",
    "            continue\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"\\tBatch: \", i)\n",
    "        # print(batch)\n",
    "        batch.to(device)\n",
    "        # print(batch.keys())\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        output = model(**batch)\n",
    "        logits = output.logits\n",
    "        loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "        print(batch.keys())\n",
    "        print(loss.detach().item())\n",
    "        print(output.logits.shape)\n",
    "        print(output.probas)\n",
    "\n",
    "        if i > 3:\n",
    "            continue\n",
    "\n",
    "    epoch_loss = sum(losses)/len(hf_time_1_train_loader)\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        for i, batch in enumerate(hf_time_1_validation_loader):\n",
    "            if i > 0:\n",
    "                continue\n",
    "            batch.to(device)\n",
    "            batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "            logits = output.logits\n",
    "            val_loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "            val_losses.append(val_loss.detach().item())\n",
    "\n",
    "        val_loss_epoch = sum(val_losses)/len(hf_time_1_validation_loader)\n",
    "        print(f\"Epoch {epoch} Validation Loss: {val_loss_epoch}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "for i, test_batch in enumerate(hf_time_1[\"test\"]):\n",
    "\n",
    "    if i > 0:\n",
    "        break\n",
    "    \n",
    "    text = test_batch[\"formatted_prompt\"]\n",
    "    tokenized_chat_template, messages_list = preprocess_and_tokenize(text, label=False, add_generation_prompt=True, output_messages_list=True)\n",
    "    output = model.generate(**tokenized_chat_template.to(device))\n",
    "    pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(text)\n",
    "    print(tokenized_chat_template)\n",
    "    print(output)\n",
    "    print(pred)\n",
    "\n",
    "print(\"CHECKING GENERATION\")\n",
    "print(messages_list)\n",
    "\n",
    "print(tokenized_chat_template)\n",
    "print(output)\n",
    "\n",
    "print(type(output))\n",
    "print(output.shape)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Saving the model and Tokenizer\")\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "model.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "tokenizer.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "\n",
    "print(\"RUN SUCCESSFULLY\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

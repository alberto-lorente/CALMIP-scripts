{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1e57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import sys \n",
    "import warnings \n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import whoami, HfFolder\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3e640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1deefbe",
   "metadata": {},
   "source": [
    "## Checking whats wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17cd9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import sys \n",
    "import warnings \n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "from dotenv import load_dotenv\n",
    "from datetime import date\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import emoji\n",
    "import json\n",
    "from huggingface_hub import whoami, HfFolder\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss, Softmax, KLDivLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "432a4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llm_experiments_set_up.json\", \"r\") as f:\n",
    "    exp_setup = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c2645a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=None\n",
    "batch_size=16\n",
    "type_experiment=\"from_expl_to_impl\"\n",
    "cl_technique=\"ewc\"\n",
    "model_id = \"Models/SmolLM2-360M-Instruct\"\n",
    "training_order=[\"explicit_hs\", \"implicit_hs\"]\n",
    "testing_order=[\"explicit_hs\", \"implicit_hs\"]\n",
    "batch_size = batch_size\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "lora_r = 8\n",
    "exp_setup = exp_setup\n",
    "mode = None\n",
    "dataset_path=\"df_from_exp_to_imp.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161578e",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "646a7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLTechniques:\n",
    "    \"\"\"Container for all continual learning techniques\"\"\"\n",
    "\n",
    "    def __init__(self, model, device, technique=\"none\",\n",
    "                ewc_lambda=1000,\n",
    "                mem_size=100,\n",
    "                lwf_lambda=1,\n",
    "                temperature=2,\n",
    "                mas_lambda=1000):\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.technique = technique.lower()\n",
    "\n",
    "        # Initialize selected technique\n",
    "        if self.technique == \"ewc\":\n",
    "            self._init_ewc(ewc_lambda)\n",
    "        elif self.technique == \"agem\":\n",
    "            self._init_agem(mem_size)\n",
    "        elif self.technique == \"lwf\":\n",
    "            self._init_lwf(lwf_lambda, temperature)\n",
    "        elif self.technique == \"mas\":\n",
    "            self._init_mas(mas_lambda)\n",
    "\n",
    "    def _init_ewc(self, ewc_lambda):\n",
    "        \"\"\"Elastic Weight Consolidation\"\"\"\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.params = {n: p.clone().detach()\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if p.requires_grad}\n",
    "        self.fisher = {n: torch.zeros_like(p)\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if p.requires_grad}\n",
    "\n",
    "    def _init_agem(self, mem_size):\n",
    "        \"\"\"Average Gradient Episodic Memory\"\"\"\n",
    "        self.mem_size = mem_size\n",
    "        self.memory = []\n",
    "\n",
    "    def _init_lwf(self, lwf_lambda, temperature):\n",
    "        \"\"\"Learning Without Forgetting\"\"\"\n",
    "        self.lwf_lambda = lwf_lambda\n",
    "        self.temperature = temperature\n",
    "        self.old_model = None\n",
    "\n",
    "    def _init_mas(self, mas_lambda):\n",
    "        \"\"\"Memory Aware Synapses\"\"\"\n",
    "        self.mas_lambda = mas_lambda\n",
    "        self.importance = {n: torch.zeros_like(p)\n",
    "                        for n, p in self.model.named_parameters()\n",
    "                        if p.requires_grad}\n",
    "        self.old_params = deepcopy(self.importance)\n",
    "\n",
    "    def compute_regularization(self, inputs=None):\n",
    "        \"\"\"Compute CL regularization term\"\"\"\n",
    "        if self.technique == \"ewc\":\n",
    "            penalty = 0\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    penalty += (self.fisher[n] * (p - self.params[n]).pow(2)).sum()\n",
    "            return self.ewc_lambda * penalty\n",
    "\n",
    "        elif self.technique == \"lwf\" and self.old_model:\n",
    "            with torch.no_grad():\n",
    "                logits = inputs['logits']\n",
    "                actual_inputs = {k:torch.squeeze(v).to(self.device) for k,v in inputs.items() if k != \"logits\"}\n",
    "                old_outputs = self.old_model(**actual_inputs)\n",
    "            return self.lwf_lambda * KLDivLoss(reduction='batchmean')(\n",
    "                torch.log_softmax(logits/self.temperature, dim=1),\n",
    "                torch.softmax(old_outputs.logits/self.temperature, dim=1)\n",
    "            ) * (self.temperature ** 2)\n",
    "\n",
    "        elif self.technique == \"mas\":\n",
    "            penalty = 0\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    penalty += (self.importance[n] * (p - self.old_params[n]).pow(2)).sum()\n",
    "            return self.mas_lambda * penalty\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def pre_backward(self, inputs=None):\n",
    "        \"\"\"Operations before backward pass\"\"\"\n",
    "        if self.technique == \"agem\" and self.memory:\n",
    "            # Store current gradient\n",
    "            self.model.zero_grad()\n",
    "            for inputs_mem, labels_mem in self.memory:\n",
    "                inputs_mem = {k:torch.squeeze(v).to(self.device) for k,v in inputs_mem.items()}\n",
    "                labels_mem = torch.squeeze(labels_mem).to(self.device)\n",
    "                outputs = self.model(**inputs_mem)\n",
    "                loss = loss_f(outputs.logits, labels_mem)\n",
    "                loss.backward()\n",
    "\n",
    "            self.ref_grad = [p.grad.clone() for p in self.model.parameters() if p.requires_grad] # i think this should be with req grad\n",
    "            self.model.zero_grad()\n",
    "\n",
    "    def post_backward(self):\n",
    "        \"\"\"Operations after backward pass\"\"\"\n",
    "        if self.technique == \"agem\" and hasattr(self, 'ref_grad'):\n",
    "            # Project gradients\n",
    "            dot_product = sum(torch.sum(p.grad * g_ref)\n",
    "                        for p, g_ref in zip(self.model.parameters(), self.ref_grad) if p.requires_grad)\n",
    "            ref_norm = sum(torch.sum(g_ref * g_ref) for g_ref in self.ref_grad)\n",
    "\n",
    "            if dot_product < 0:  # Negative interference\n",
    "                scale = dot_product / (ref_norm + 1e-8)\n",
    "                for p, g_ref in zip(self.model.parameters(), self.ref_grad):\n",
    "                    if p.grad is not None and p.requires_grad:\n",
    "                        p.grad -= scale * g_ref\n",
    "\n",
    "    def post_task_update(self, dataloader=None):\n",
    "        \"\"\"Update after each task\"\"\"\n",
    "        if self.technique == \"ewc\":\n",
    "            # Compute Fisher information\n",
    "            self.model.eval()\n",
    "            for batch in dataloader:\n",
    "                self.model.zero_grad()\n",
    "                inputs = {k:torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                # outputs = self.model(**inputs, labels=labels)\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "                loss = loss_f(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        self.fisher[n] += p.grad.pow(2) / len(dataloader)\n",
    "\n",
    "            # Update stored parameters\n",
    "            self.params = {n: p.clone().detach()\n",
    "                        for n, p in self.model.named_parameters()\n",
    "                        if p.requires_grad}\n",
    "\n",
    "        elif self.technique == \"agem\":\n",
    "            # Update memory buffer\n",
    "            self.memory = []\n",
    "            for batch in dataloader:\n",
    "                inputs = {k: torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "                labels = torch.squeeze(batch['labels']).to(self.device)\n",
    "                self.memory.append((inputs, labels))\n",
    "                if len(self.memory) >= self.mem_size:\n",
    "                    break\n",
    "\n",
    "        elif self.technique == \"lwf\":\n",
    "            # Save model snapshot\n",
    "            self.old_model = deepcopy(self.model)\n",
    "            self.old_model.eval()\n",
    "\n",
    "        elif self.technique == \"mas\":\n",
    "            # Update importance weights\n",
    "            self.model.eval()\n",
    "            for batch in dataloader:\n",
    "                self.model.zero_grad()\n",
    "                inputs = {k: torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                # does this need a loss?????????????\n",
    "                torch.norm(outputs.logits, p=2, dim=1).mean().backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        self.importance[n] += p.grad.abs() / len(dataloader)\n",
    "\n",
    "            # Update stored parameters\n",
    "            self.old_params = {n: p.clone().detach()\n",
    "                            for n, p in self.model.named_parameters()\n",
    "                            if p.requires_grad}\n",
    "\n",
    "class AutoContinualLearner(nn.Module):\n",
    "    def __init__(self, model_name, device, quantization_config, torch_dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch_dtype\n",
    "        )\n",
    "        self.n_initial_params = sum(t.numel() for t in self.model.parameters())\n",
    "        self.n_trainable_params_initial = sum(t.numel() for t in self.model.parameters() if t.requires_grad)\n",
    "        self.cl = None\n",
    "\n",
    "    def init_cl(self, technique, lora_config, **kwargs):\n",
    "        \"\"\"Init the continual learning technique\"\"\"\n",
    "        self.model = get_peft_model(self.model, lora_config).to(self.device)\n",
    "        self.n_params_lora = sum(t.numel() for t in self.model.parameters())\n",
    "        self.n_trainable_params_lora = sum(t.numel() for t in self.model.parameters() if t.requires_grad)\n",
    "        self.model.print_trainable_parameters()\n",
    "        self.cl = CLTechniques(self.model, self.device, technique, **kwargs)\n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f2759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_failed_batch(batch):\n",
    "    \n",
    "    print(\"FAILED UNSQUEEZED BATCH\")\n",
    "    if \"failed_batches.json\" not in list(os.listdir()):\n",
    "        with open(\"failed_batches.json\", \"w\") as f:\n",
    "            json.dump([], f)\n",
    "    with open(\"failed_batches.json\", \"r\") as f:\n",
    "        failed_batches = json.load(f)\n",
    "    failed_batches.append(batch)\n",
    "    with open(\"failed_batches.json\", \"w\") as f:\n",
    "        json.dump(failed_batches, f)\n",
    "\n",
    "def log_hf():\n",
    "    \n",
    "    load_dotenv(\"env_vars.env\")\n",
    "    hf_token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    HfFolder.save_token(hf_token)\n",
    "    return print(whoami()[\"name\"])\n",
    "\n",
    "def setup():\n",
    "    try:\n",
    "        dist.init_process_group(\"nccl\")\n",
    "        # dist.init_process_group(\"gloo\")\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        return local_rank\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"DISTR TRAINING ALREADY INITIALIZED\")\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        return local_rank\n",
    "\n",
    "\n",
    "def save_results_csv(df, experiment_name, model_id, cl_technique, result_type=\"specific\"):\n",
    "\n",
    "    cl_technique_clean = cl_technique.replace(\" + \", \"__\")\n",
    "    id_ = model_id.replace(\"/\", \"-\") + \"_\" + cl_technique_clean + \"_\" + str(date.today())\n",
    "    id_clean = experiment_name + id_.replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\",\"-\") + result_type + \".csv\"\n",
    "    df.to_csv(id_clean, index=False)\n",
    "    return print(\"Saved in path: \", id_clean)\n",
    "\n",
    "def clean_cl_name(cl_name):\n",
    "\n",
    "    regex = r'<(?:[\\w\\.]+)?\\.([\\w]+) object at'\n",
    "    matches =   re.findall(regex, cl_name)\n",
    "    clean_string = \" + \".join(matches)\n",
    "    return clean_string\n",
    "\n",
    "def clean_metric_name(metric_name):\n",
    "\n",
    "    reg = r\"\\s([a-z_1]+)\\s\"\n",
    "    match_ = re.search(reg, metric_name)\n",
    "    clean_str = match_.group().strip()\n",
    "\n",
    "    return clean_str\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n",
    "\n",
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def get_probability_distribution(logits):\n",
    "    probability_dist = Softmax(dim=-1)(logits)\n",
    "    return probability_dist\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100) # ignore the left pad tokens\n",
    "def loss_f(logits, labels):\n",
    "\n",
    "    flat_logits = logits.view(-1, logits.size(-1))\n",
    "    flat_labels = labels.view(-1)\n",
    "\n",
    "    loss = loss_fn(flat_logits, flat_labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def translate_prediction_to_label(text):\n",
    "    if \"NOT HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    elif \"NOT_HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT_HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else: \n",
    "            return 0\n",
    "    elif \"HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"HATEFUL\", \"\")\n",
    "        if \"NOT_HATEFUL\" in text_clean or \"NOT HATEFUL\" in text_clean:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4891c086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Preapring the Tokenizer\n",
      "----------Preparing the Data-----------------\n",
      "_________________________________\n",
      "Loading and filtering the Data\n",
      "dataset\n",
      "dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 862.47 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:06<00:00, 861.21 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 878.18 examples/s]\n",
      "Map: 100%|██████████| 1019/1019 [00:01<00:00, 841.46 examples/s]\n",
      "Map: 100%|██████████| 8155/8155 [00:09<00:00, 857.43 examples/s]\n",
      "Map: 100%|██████████| 1020/1020 [00:01<00:00, 886.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "########################################################## DATA WORK\n",
    "print(\"_________________________________\")\n",
    "print(\"Preapring the Tokenizer\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id + \"/Tokenizer\")\n",
    "if tokenizer.pad_token is None and \"Llama\" in model_id: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "elif tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = open(model_id + \"/Tokenizer/chat_template.jinja\").read()\n",
    "\n",
    "# print(tokenizer.chat_template)\n",
    "# print(tokenizer.apply_chat_template(\"Hello World\", tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"))\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=512):\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # for the cross entropy loss\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "print(\"----------Preparing the Data-----------------\")\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading and filtering the Data\")\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "times_array = list(df[\"time\"].unique())\n",
    "datasets = []\n",
    "dataset_names = list(df[\"task\"].unique())\n",
    "\n",
    "for task in training_order:\n",
    "\n",
    "    time_ds = []\n",
    "    for split in df[\"split\"].unique():\n",
    "\n",
    "        split_df = df[(df[\"split\"] == split) & (df[\"task\"] == task)]\n",
    "        hf_split = Dataset.from_pandas(split_df)\n",
    "        time_ds.append(hf_split)\n",
    "    datasets.append(time_ds)\n",
    "\n",
    "datasets_test = []\n",
    "for i, task in enumerate(testing_order):\n",
    "\n",
    "    split_df = df[(df[\"split\"] == \"test\") & (df[\"task\"] == task)]\n",
    "    hf_split = Dataset.from_pandas(split_df)\n",
    "    datasets_test.append({testing_order[i]: hf_split})\n",
    "\n",
    "hf_datasets = []\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(\"dataset\")\n",
    "\n",
    "    hf_ds = DatasetDict({dataset[0][\"split\"][0]: dataset[0], \n",
    "                        dataset[1][\"split\"][0]: dataset[1],\n",
    "                        dataset[2][\"split\"][0]: dataset[2]})\n",
    "    hf_ds_name = training_order[i]\n",
    "    hf_datasets.append({hf_ds_name: hf_ds})\n",
    "\n",
    "hf_datasets_processed = []\n",
    "for ds in hf_datasets:\n",
    "    ds_dict = {}\n",
    "    for task_name, hf_data in ds.items():\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            ds_dict[task_name][split] = hf_data[split].map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)\n",
    "    hf_datasets_processed.append(ds_dict)\n",
    "\n",
    "n_samples_per_ds = [\n",
    "    len(hf_time[\"train\"])\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items() \n",
    "]\n",
    "\n",
    "for ds in hf_datasets_processed:\n",
    "    for task_name, hf_data in ds.items():\n",
    "        for split in hf_data:\n",
    "            hf_data[split].set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \n",
    "                \"target\", \"implied_statement\", \"split\", \"time\", \"task\",\n",
    "                \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "hf_datasets_no_cols = []\n",
    "for ds in hf_datasets_processed:\n",
    "    ds_dict = {}\n",
    "    for task_name, hf_data in ds.items():\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            if split != \"test\":\n",
    "                ds_dict[task_name][split] = hf_data[split].remove_columns(cols_to_remove)\n",
    "    hf_datasets_no_cols.append(ds_dict)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e047325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADERS AT THE END OF PROCESSING\n",
      "[{'explicit_hs': {'train': <torch.utils.data.dataloader.DataLoader object at 0x000001C620F24E60>,\n",
      "                  'validation': <torch.utils.data.dataloader.DataLoader object at 0x000001C620F7C680>},\n",
      "  'test': Dataset({\n",
      "    features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
      "    num_rows: 720\n",
      "})},\n",
      " {'implicit_hs': {'train': <torch.utils.data.dataloader.DataLoader object at 0x000001C620F259A0>,\n",
      "                  'validation': <torch.utils.data.dataloader.DataLoader object at 0x000001C620F24110>},\n",
      "  'test': Dataset({\n",
      "    features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
      "    num_rows: 1020\n",
      "})}]\n",
      "\n",
      "TEST DATA AT THE END OF PROCESSING\n",
      "[{'explicit_hs': Dataset({\n",
      "    features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
      "    num_rows: 720\n",
      "})},\n",
      " {'implicit_hs': Dataset({\n",
      "    features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
      "    num_rows: 1020\n",
      "})}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_loaders = []\n",
    "for ds in hf_datasets_no_cols:\n",
    "    ds_dict = {}\n",
    "    for task_name, hf_data in ds.items():\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            if split != \"test\":\n",
    "                data_loader = DataLoader(ds[task_name][split], collate_fn=data_collator, batch_size=batch_size)\n",
    "                ds_dict[task_name][split] = data_loader\n",
    "    data_loaders.append(ds_dict)\n",
    "\n",
    "for i, ds in enumerate(hf_datasets):\n",
    "    for task_name, hf_data in ds.items():\n",
    "        for split in hf_data:\n",
    "            if split == \"test\":\n",
    "                data_loaders[i][\"test\"] = hf_data[split]\n",
    "\n",
    "print(\"DATA LOADERS AT THE END OF PROCESSING\")\n",
    "pp(data_loaders)\n",
    "print()\n",
    "print(\"TEST DATA AT THE END OF PROCESSING\")\n",
    "pp(datasets_test)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b630b",
   "metadata": {},
   "source": [
    "## Preparing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db84575",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_array = []\n",
    "\n",
    "for i in range(len(training_order)):\n",
    "    epochs_array.append(n_epochs)\n",
    "\n",
    "ks_array = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab1b6c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Loading the model and model config with LoRA and 4-bit quantization nf4\n",
      "trainable params: 1,638,400 || all params: 363,459,520 || trainable%: 0.4508\n",
      "_________________________________\n",
      "CONTINUAL LEARNING EXPERIMENT SET UP\n",
      "model :\t Models/SmolLM2-360M-Instruct\n",
      "\n",
      "training_order :\t ['explicit_hs', 'implicit_hs']\n",
      "\n",
      "testing_order :\t ['explicit_hs', 'implicit_hs']\n",
      "\n",
      "zero_testing :\t []\n",
      "\n",
      "epochs :\t [8, 8]\n",
      "\n",
      "cl_technique :\t ewc\n",
      "\n",
      "type_experiment :\t from_expl_to_impl\n",
      "\n",
      "batch_size :\t 16\n",
      "\n",
      "learning_rate :\t 0.0001\n",
      "\n",
      "results :\t []\n",
      "\n",
      "_________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Loading the model and model config with LoRA and 4-bit quantization nf4\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n",
    "\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "if cl_technique in [\"ewc\", \"agem\", \"lwf\", \"mas\"]:\n",
    "    cl_hyperparams = {\n",
    "    \"ewc\": {\"ewc_lambda\":1500},\n",
    "    \"agem\": {\"mem_size\":100},\n",
    "    \"lwf\": {\"lwf_lambda\":1,\n",
    "            \"temperature\":2},\n",
    "    \"mas\": {\"mas_lambda\":1000}\n",
    "    }\n",
    "\n",
    "    cl_params = cl_hyperparams[cl_technique]\n",
    "    hyper_param_str = \"=\".join([str(k) + \"-\" + str(v) for k, v in cl_params.items()])\n",
    "\n",
    "    model = AutoContinualLearner(model_id + \"/Model\", device, bnb_config)\n",
    "    model.init_cl(technique=cl_technique, lora_config=config, **cl_params)\n",
    "\n",
    "else:\n",
    "    cl_params = {\"NA\": \"NA\"}\n",
    "    hyper_param_str = \"NA\"\n",
    "\n",
    "    model = AutoContinualLearner(model_id + \"/Model\", device, bnb_config)\n",
    "    model.init_cl(technique=cl_technique, lora_config=config)\n",
    "\n",
    "n_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "\n",
    "hf_datasets = data_loaders\n",
    "\n",
    "zero_testing_order = [dataset for dataset in testing_order if dataset not in training_order]\n",
    "\n",
    "results = {\n",
    "            \"model\": model_id,\n",
    "            \"training_order\": training_order,\n",
    "            \"testing_order\": testing_order,\n",
    "            \"zero_testing\":zero_testing_order,\n",
    "            \"epochs\": epochs_array,\n",
    "            \"exp_setup\": exp_setup,\n",
    "            \"cl_technique\": cl_technique,\n",
    "            \"type_experiment\": type_experiment,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": lr,\n",
    "            \"results\": []\n",
    "            }\n",
    "\n",
    "print(\"CONTINUAL LEARNING EXPERIMENT SET UP\")\n",
    "for k, v in results.items():\n",
    "    if k != \"exp_setup\":\n",
    "        print(k, \":\\t\", v)\n",
    "        print()\n",
    "\n",
    "data_loaders_train = []\n",
    "data_loaders_val = []\n",
    "test_datasets = []\n",
    "for time, ds in enumerate(training_order):\n",
    "    # print(\"training order\")\n",
    "    # print(training_order)\n",
    "    # print(\"ds\")\n",
    "    # print(ds)\n",
    "    # print(\"hf_datasets\")\n",
    "    # print(hf_datasets[time].keys())\n",
    "    data_loaders_train.append(hf_datasets[time][ds][\"train\"])\n",
    "    data_loaders_val.append(hf_datasets[time][ds][\"validation\"])\n",
    "    test_datasets.append({\"test\":hf_datasets[time][\"test\"]})\n",
    "# print(\"TEST DATASETS BEFORE STARTING THE EXPERIENCE\")\n",
    "# print(test_datasets)\n",
    "\n",
    "test_results = []\n",
    "train_results = []\n",
    "\n",
    "n_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "\n",
    "hf_datasets = data_loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "203f45ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': Dataset({\n",
       "     features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
       "     num_rows: 720\n",
       " })}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "705ec67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_notneeded_dimension(x):\n",
    "    # print(\"X SHAPE\")\n",
    "    print(x.shape)\n",
    "    x = x.squeeze(1) if x.dim() == 3 and x.size(1) == 1 else x\n",
    "    # print(\"X NEW SHAPE\")\n",
    "    # print(x.shape)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83e35d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Testing the model\n",
      "326\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: moves to tackle serious #immigration issue - hearing on asylum abuse by end of the year #noamnesty\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 98])\n",
      "torch.Size([1, 98])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,  7488,   288, 10848,  3199,\n",
      "           907,  9086, 10457,  2843,   731,  5364,   335, 24887,  5648,   411,\n",
      "          1112,   282,   260,   713,   907,  4607,   332, 33971,   198, 44724,\n",
      "          7485, 12111, 40560,    42,   469,  3124,   868,   325,   915,   260,\n",
      "          4368,    30,     2,   198,     1,   520,  9531,   198, 18083,   407,\n",
      "          9081,    54,  6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "332\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: reactionary nationalism will not salvage the working class of the world . capitalism can only be ended t\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 95])\n",
      "torch.Size([1, 95])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,  5303,   529, 18726,   523,\n",
      "           441, 37098,   260,  1891,  1128,   282,   260,   905,  1673, 15262,\n",
      "           416,   805,   325,  7569,   252,   198, 44724,  7485, 12111, 40560,\n",
      "            42,   469,  3124,   868,   325,   915,   260,  4368,    30,     2,\n",
      "           198,     1,   520,  9531,   198, 18083,   407,  9081,    54,  6565,\n",
      "             2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "276\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: they aren't white supremacists are you kidding ?\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 88])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,   502,  4775,   982,  2537,\n",
      "         13635, 17299,   911,   359,   346,   501, 24471,  9148,   198, 44724,\n",
      "          7485, 12111, 40560,    42,   469,  3124,   868,   325,   915,   260,\n",
      "          4368,    30,     2,   198,     1,   520,  9531,   198, 18083,   407,\n",
      "          9081,    54,  6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "256\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: a history of white privilege\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 81])\n",
      "torch.Size([1, 81])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,   253,  1463,   282,  2537,\n",
      "         16071,   198, 44724,  7485, 12111, 40560,    42,   469,  3124,   868,\n",
      "           325,   915,   260,  4368,    30,     2,   198,     1,   520,  9531,\n",
      "           198, 18083,   407,  9081,    54,  6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "288\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: will hillary disavow her white racist supporters ? why not ?\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 90])\n",
      "torch.Size([1, 90])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,   523, 13083,   529,   510,\n",
      "           565,   323,   874,  2537, 21852, 15922,  9148,  1701,   441,  9148,\n",
      "           198, 44724,  7485, 12111, 40560,    42,   469,  3124,   868,   325,\n",
      "           915,   260,  4368,    30,     2,   198,     1,   520,  9531,   198,\n",
      "         18083,   407,  9081,    54,  6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "303\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: all the volcanoes are in that area  you want to genocide us white ppl ? lol\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 95])\n",
      "torch.Size([1, 95])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,   511,   260, 20920,   359,\n",
      "           281,   338,  1557,   216,   346,  1277,   288, 19128,   468,  2537,\n",
      "           273,   439,  9148,   303,   308,   198, 44724,  7485, 12111, 40560,\n",
      "            42,   469,  3124,   868,   325,   915,   260,  4368,    30,     2,\n",
      "           198,     1,   520,  9531,   198, 18083,   407,  9081,    54,  6565,\n",
      "             2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "270\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: you got any of that rat jew robert spencer\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 87])\n",
      "torch.Size([1, 87])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,   346,  3363,   750,   282,\n",
      "           338,  4309, 11159,   651,  6338,   574, 22956,   198, 44724,  7485,\n",
      "         12111, 40560,    42,   469,  3124,   868,   325,   915,   260,  4368,\n",
      "            30,     2,   198,     1,   520,  9531,   198, 18083,   407,  9081,\n",
      "            54,  6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "314\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: india is passing through a sad phase ; how can any yadav be friends with beef eaters ?\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 96])\n",
      "torch.Size([1, 96])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42, 47493,   314,  7685,   738,\n",
      "           253,  8177,  5239,  8544,   638,   416,   750,   329,   344,   565,\n",
      "           325,  2428,   351, 12418, 39869,  9148,   198, 44724,  7485, 12111,\n",
      "         40560,    42,   469,  3124,   868,   325,   915,   260,  4368,    30,\n",
      "             2,   198,     1,   520,  9531,   198, 18083,   407,  9081,    54,\n",
      "          6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "314\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: we need your help to get back to 100 % in the fight against amnesty and illegals ! ...\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,   392,   737,   469,   724,\n",
      "           288,   820,  1056,   288,   216,    33,    32,    32,  2471,   281,\n",
      "           260,  3568,  1523,   744, 33971,   284, 24991,   781,  5728,  4563,\n",
      "           198, 44724,  7485, 12111, 40560,    42,   469,  3124,   868,   325,\n",
      "           915,   260,  4368,    30,     2,   198,     1,   520,  9531,   198,\n",
      "         18083,   407,  9081,    54,  6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "340\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: i've never done squats with a barbell or any deadlifting at all but the rest i'm more or less comfortable with .\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 103])\n",
      "torch.Size([1, 103])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,  2056,  3543,  2093,  2294,\n",
      "          3031,  1457,   351,   253,  2753, 10160,   355,   750,  3895, 46343,\n",
      "           418,   511,   564,   260,  1532,  2056,  5248,   540,   355,  1181,\n",
      "          6761,   351,  1673,   198, 44724,  7485, 12111, 40560,    42,   469,\n",
      "          3124,   868,   325,   915,   260,  4368,    30,     2,   198,     1,\n",
      "           520,  9531,   198, 18083,   407,  9081,    54,  6565,     2]],\n",
      "       device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "331\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: strict measures have to be taken because black money is stashed in more than 70 countries : baba ramdev\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,  6844,  3736,   457,   288,\n",
      "           325,  2473,   975,  2632,  2478,   314,   328, 12165,   281,   540,\n",
      "           670,   216,    39,    32,  2075,  1577,  3383,    81, 15327,  6301,\n",
      "           198, 44724,  7485, 12111, 40560,    42,   469,  3124,   868,   325,\n",
      "           915,   260,  4368,    30,     2,   198,     1,   520,  9531,   198,\n",
      "         18083,   407,  9081,    54,  6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "352\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: muslims are not being quiet about it any more they're telling people they are taking over state local and federal government\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
      "             1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
      "            30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
      "          1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
      "          4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
      "          6565,    30,   198, 26826, 41601,    42,  1424,  3219,    99,   359,\n",
      "           441,  1036,  8354,   563,   357,   750,   540,   502,  2316,  8932,\n",
      "           701,   502,   359,  2498,   690,  1215,  1679,   284,  4251,  1693,\n",
      "           198, 44724,  7485, 12111, 40560,    42,   469,  3124,   868,   325,\n",
      "           915,   260,  4368,    30,     2,   198,     1,   520,  9531,   198,\n",
      "         18083,   407,  9081,    54,  6565,     2]], device='cuda:0')\n",
      "Prediction:\n",
      "NOT HATEFUL\n",
      "\n",
      "354\n",
      "CHAT TEMPLATE COMPUTED\n",
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "You are a social media content moderator.\n",
      "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
      "MESSAGE: pa and driver of munde know much more than is presently presumed . offenders are well connected and r able to divert attention\n",
      "OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m input_ids_tokenized \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     38\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m input_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids_tokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[0;32m     48\u001b[0m seq \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\peft_model.py:1838\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1836\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1837\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1838\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1840\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2594\u001b[0m     )\n\u001b[0;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2614\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3564\u001b[0m     outputs,\n\u001b[0;32m   3565\u001b[0m     model_kwargs,\n\u001b[0;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3567\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m    685\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:308\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:277\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[1;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    267\u001b[0m     query_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    274\u001b[0m )\n\u001b[0;32m    276\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 277\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:518\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    515\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[1;32m--> 518\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlora_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m scaling\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dropout, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mIdentity) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "\n",
    "predictions_test = []\n",
    "labels_test = []\n",
    "predicted_strings = []\n",
    "labels_strings = []\n",
    "full_generation = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_datasets[0].values()):\n",
    "        for test_item in data:\n",
    "            target_label = test_item[\"label\"]\n",
    "            labels_strings.append(target_label)\n",
    "            if target_label == \"NOT HATEFUL\":\n",
    "                target_label = 0\n",
    "            elif target_label == \"HATEFUL\":\n",
    "                target_label = 1\n",
    "            labels_test.append(target_label)\n",
    "\n",
    "            formatted_prompt = test_item[\"formatted_prompt\"]\n",
    "            # prompt_plus_messages = base_prompt.format(clean_post)\n",
    "            # print(\"FORMATTED PROMPT\")\n",
    "            # print(formatted_prompt)\n",
    "\n",
    "            pp(len(formatted_prompt))\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "                {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "            ]\n",
    "            chat_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            print(\"CHAT TEMPLATE COMPUTED\")\n",
    "            print(chat_template)\n",
    "            input_dict = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False)\n",
    "            input_dict = {k: squeeze_notneeded_dimension(v).to(device) for k, v in input_dict.items()}\n",
    "            input_ids_tokenized = input_dict[\"input_ids\"]\n",
    "            attention_mask = input_dict[\"attention_mask\"]\n",
    "            \n",
    "            output = model.model.generate(input_ids=input_ids_tokenized, \n",
    "                                            attention_mask=attention_mask, \n",
    "                                            top_p=0.9, \n",
    "                                            temperature=0.6, \n",
    "                                            max_new_tokens=10,\n",
    "                                            return_dict_in_generate=False)\n",
    "            print(output)\n",
    "                    \n",
    "            seq = output[0]\n",
    "            pred = tokenizer.decode(seq[input_ids_tokenized.shape[1]:], skip_special_tokens=True)\n",
    "            print(\"Prediction:\")\n",
    "            print(pred)\n",
    "            full_generation.append(pred)\n",
    "            predicted_strings.append(pred)\n",
    "            pred_label = translate_prediction_to_label(pred)\n",
    "            predictions_test.append(pred_label)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "514579e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Starting Experience----------------\n",
      "------------------TIME 0------------------------\n",
      "\n",
      "Epochs in the current time: 8\n",
      "Number of training samples: 5752\n",
      "Current Dataset: explicit_hs\n",
      "\n",
      "_________________________________\n",
      "Training the model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n",
      "input_ids: torch.Size([16, 511])\n",
      "\n",
      "labels: torch.Size([16, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 511])\n",
      "\n",
      "input_ids: torch.Size([16, 1, 511])\n",
      "\n",
      "labels: torch.Size([16, 1, 511])\n",
      "\n",
      "attention_mask: torch.Size([16, 1, 511])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 14/360 [00:26<10:48,  1.87s/it]\n",
      "  0%|          | 0/8 [00:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:324\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m--> 324\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:162\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 162\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:432\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2380\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 2380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.38 GiB is allocated by PyTorch, and 195.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 83\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# print(\"Unsqueezed Batch\")\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# for k, v in batch_unsqueezed.items():\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m#     print(f\"{k}: {v.shape}\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# print(batch[\"attention_mask\"].shape)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# print(batch[\"labels\"].shape)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# print(output)\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\peft_model.py:1717\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[0;32m   1715\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[1;32m-> 1717\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enable_peft_forward_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspecial_peft_forward_args\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:141\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, typ, value, traceback):\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for time, current_training_dataset in enumerate(data_loaders_train): # current tr_ds is a string!!\n",
    "    print(\"------------------Starting Experience----------------\")\n",
    "    print(f\"------------------TIME {time}------------------------\")\n",
    "    print()\n",
    "    # torch.cuda.ipc_collect()\n",
    "    # torch.cuda.empty_cache()\n",
    "    n_epochs = epochs_array[time]\n",
    "    if ks_array != None:\n",
    "        n_samples = ks_array[time]\n",
    "    else:\n",
    "        n_samples = n_samples_per_ds[time]\n",
    "\n",
    "    current_dataset_name = training_order[time]\n",
    "    current_testing_dataset = testing_order[time]\n",
    "\n",
    "    print(f\"Epochs in the current time: {n_epochs}\\nNumber of training samples: {n_samples}\\nCurrent Dataset: {current_dataset_name}\")\n",
    "    print()\n",
    "\n",
    "    train_loader=data_loaders_train[time]\n",
    "    validation_loader=data_loaders_val[time]\n",
    "\n",
    "\n",
    "    print(\"_________________________________\")\n",
    "    print(\"Training the model\")\n",
    "    print()\n",
    "\n",
    "    global_training_losses = []\n",
    "    global_validation_losses = []\n",
    "    current_testing_dataset=testing_order[time]\n",
    "\n",
    "    model.train()\n",
    "    with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "    # for task in tasks/dataset - train, eval\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            if epoch > 0:\n",
    "                continue\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            epoch_validation_losses = []\n",
    "            train_losses = []\n",
    "\n",
    "            print(\"Epoch: \", epoch)\n",
    "\n",
    "            for i, batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                batch_unsqueezed = batch\n",
    "                # print(\"\\tBatch: \", i)\n",
    "                batch = {k:torch.squeeze(v, dim=1).to(device) for k,v in batch.items()}\n",
    "\n",
    "                # print(\"Squeezed Batch\")\n",
    "                for k, v in batch.items():\n",
    "                    if v.shape[0] != batch_size:\n",
    "                        print(f\"{k}: {v.shape}\")\n",
    "                        print()\n",
    "                        continue\n",
    "\n",
    "                for k, v in batch.items():\n",
    "                    print(f\"{k}: {v.shape}\")\n",
    "                    print()\n",
    "\n",
    "                for k, v in batch_unsqueezed.items():\n",
    "                    print(f\"{k}: {v.shape}\")\n",
    "                    print()\n",
    "                    continue\n",
    "\n",
    "                    continue\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "                # print(\"Unsqueezed Batch\")\n",
    "                # for k, v in batch_unsqueezed.items():\n",
    "                #     print(f\"{k}: {v.shape}\")\n",
    "\n",
    "                # print(batch[\"input_ids\"].shape)\n",
    "                # print(batch[\"attention_mask\"].shape)\n",
    "                # print(batch[\"labels\"].shape)\n",
    "                try:\n",
    "                    output = model.model(**batch)\n",
    "                    # print(output)\n",
    "                    logits = output.logits\n",
    "                    # print(\"Shape Logits\")\n",
    "                    # print(logits.shape)\n",
    "                    # print(\"Shape Labels\")\n",
    "                    # print(batch[\"labels\"].shape)\n",
    "                    loss = loss_f(logits, batch[\"labels\"])\n",
    "\n",
    "                    if i == 0:\n",
    "                        print(\"Checking that the model type is the continual learner to do the cls\")\n",
    "                        print(type(model))\n",
    "                        print(type(model.cl))\n",
    "                        print(model.cl)\n",
    "                        # print(dir(model.module))\n",
    "                    if model.cl:\n",
    "                        batch['logits'] = logits  # needed for LwF\n",
    "                        loss += modelodule.cl.compute_regularization(batch)\n",
    "                        model.cl.pre_backward(batch)\n",
    "                    # print(\"CL regularization and backward computed\")\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "\n",
    "                    # needed agem (A-GEM)\n",
    "                    if model.module.cl:\n",
    "                        model.module.cl.post_backward()\n",
    "                    # print(\"CL post backward computed\")\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    train_losses.append(loss.detach().item())\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print()\n",
    "                    print(e)\n",
    "                    print(\"Switching Batch Size to Unsqueezed\")\n",
    "\n",
    "                #     output = model.module.model(**batch_unsqueezed)\n",
    "                #     # print(output)\n",
    "                #     logits = output.logits\n",
    "                #     # print(\"Shape Logits\")\n",
    "                #     # print(logits.shape)\n",
    "                #     # print(\"Shape Labels\")\n",
    "                #     # print(batch_unsqueezed[\"labels\"].shape)\n",
    "                #     loss = loss_f(logits, batch_unsqueezed[\"labels\"])\n",
    "                #     # print(dir(model.module))\n",
    "                #     if model.module.cl:\n",
    "                #         batch_unsqueezed['logits'] = logits  # needed for LwF\n",
    "                #         loss += model.module.cl.compute_regularization(batch_unsqueezed)\n",
    "                #         model.module.cl.pre_backward(batch_unsqueezed)\n",
    "                #     print(\"CL regularization and backward computed\")\n",
    "\n",
    "                #     loss.backward()\n",
    "\n",
    "\n",
    "                #     # needed agem (A-GEM)\n",
    "                #     if model.module.cl:\n",
    "                #         model.module.cl.post_backward()\n",
    "                #     print(\"CL post backward computed\")\n",
    "\n",
    "                #     optimizer.step()\n",
    "                #     optimizer.zero_grad()\n",
    "\n",
    "                #     train_losses.append(loss.detach().item())\n",
    "\n",
    "            # epoch_loss = sum(train_losses) / len(train_losses) # loss on current device\n",
    "\n",
    "            # epoch_loss_tensor = torch.tensor(epoch_loss, device=device)\n",
    "\n",
    "            # print(f\"Epoch Loss: {epoch_loss_tensor.item()}\")\n",
    "            # global_training_losses.append(epoch_loss_tensor.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32):\n",
    "            with torch.no_grad():\n",
    "\n",
    "                print(\"_________________________________\")\n",
    "                print(\"Validating the model\")\n",
    "                print()\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                val_losses = [] # val loss for each batch\n",
    "\n",
    "                for i, batch in enumerate(tqdm(validation_loader)):\n",
    "                    \n",
    "                    # batch.to(device)\n",
    "                    batch_unsqueezed = batch\n",
    "                    # print(\"\\tBatch: \", i)\n",
    "                    batch = {k:torch.squeeze(v).to(device) for k,v in batch.items()}\n",
    "\n",
    "                    # print(\"Squeezed Batch\")\n",
    "                    for k, v in batch.items():\n",
    "                        if v.shape[0] != batch_size:\n",
    "                            print(f\"{k}: {v.shape}\")\n",
    "                            print()\n",
    "                            continue\n",
    "\n",
    "                    # print(\"Unsqueezed Batch\")\n",
    "                    # for k, v in batch_unsqueezed.items():\n",
    "                    #     print(f\"{k}: {v.shape}\")\n",
    "\n",
    "                #     try:\n",
    "                #         output = model.model(**batch)\n",
    "                #         # print(output)\n",
    "                #         logits = output.logits\n",
    "                #         # print(\"Shape Logits\")\n",
    "                #         # print(logits.shape)\n",
    "                #         # print(\"Shape Labels\")\n",
    "                #         # print(batch[\"labels\"].shape)\n",
    "                #         val_loss = loss_f(logits, batch[\"labels\"])\n",
    "                #         val_losses.append(val_loss.detach().item())\n",
    "\n",
    "                #     except Exception as e:\n",
    "                #         print(\"Switching Batch Size to unsqueezed\")\n",
    "                # #         output = model.model(**batch_unsqueezed)\n",
    "                # #         logits = output.logits\n",
    "                # #         val_loss = loss_f(logits, batch_unsqueezed[\"labels\"])\n",
    "\n",
    "                # #         val_losses.append(val_loss.detach().item())\n",
    "\n",
    "                #         print()\n",
    "                #         print(e)\n",
    "                \n",
    "                # val_loss_epoch = sum(val_losses)/len(val_losses)\n",
    "                # val_loss_tensor = torch.tensor(val_loss_epoch, device=device)\n",
    "\n",
    "                # print(\"_________________________________\")\n",
    "                # print(\"Validation completed\")\n",
    "                # print()\n",
    "                # print(f\"Validation Loss: {val_loss_tensor.item()}\")\n",
    "                # print(\"_________________________________\")\n",
    "\n",
    "                # val_loss = val_loss_tensor.item()\n",
    "                # epoch_validation_losses.append(val_loss)\n",
    "                # global_validation_losses.append(val_loss)\n",
    "\n",
    "        # print()\n",
    "        # print(\"---------------------TRAINING ENDED---------------\")\n",
    "        # print(\"Final Training Losses:\", global_training_losses)\n",
    "        # print(\"Final Validation Losses:\", global_validation_losses)\n",
    "\n",
    "        # if model.module.cl:\n",
    "        #     model.module.cl.post_task_update(train_loader)\n",
    "\n",
    "        # print(\"-----------POST TRAINING CL UPDATES COMPLETED---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b3791",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40879666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hf():\n",
    "    \n",
    "    load_dotenv(\"env_vars.env\")\n",
    "    hf_token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    HfFolder.save_token(hf_token)\n",
    "    return print(whoami()[\"name\"])\n",
    "\n",
    "def setup():\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return local_rank\n",
    "\n",
    "def save_results_csv(df, experiment_name, model_id, cl_technique, result_type=\"specific\"):\n",
    "\n",
    "    cl_technique_clean = cl_technique.replace(\" + \", \"__\")\n",
    "    id_ = model_id.replace(\"/\", \"-\") + \"_\" + cl_technique_clean + \"_\" + str(date.today())\n",
    "    id_clean = experiment_name + id_.replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\",\"-\") + result_type + \".csv\"\n",
    "    df.to_csv(id_clean, index=False)\n",
    "    return print(\"Saved in path: \", id_clean)\n",
    "\n",
    "def clean_cl_name(cl_name):\n",
    "\n",
    "    regex = r'<(?:[\\w\\.]+)?\\.([\\w]+) object at'\n",
    "    matches =   re.findall(regex, cl_name)\n",
    "    clean_string = \" + \".join(matches)\n",
    "    return clean_string\n",
    "\n",
    "def clean_metric_name(metric_name):\n",
    "\n",
    "    reg = r\"\\s([a-z_1]+)\\s\"\n",
    "    match_ = re.search(reg, metric_name)\n",
    "    clean_str = match_.group().strip()\n",
    "\n",
    "    return clean_str\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n",
    "\n",
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def get_probability_distribution(logits):\n",
    "    probability_dist = Softmax(dim=-1)(logits)\n",
    "    return probability_dist\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100) # ignore the left pad tokens\n",
    "def loss_f(logits, labels):\n",
    "\n",
    "    flat_logits = logits.view(-1, logits.size(-1))\n",
    "    flat_labels = labels.view(-1)\n",
    "\n",
    "    loss = loss_fn(flat_logits, flat_labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def translate_prediction_to_label(text):\n",
    "    if \"NOT HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    elif \"NOT_HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT_HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else: \n",
    "            return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76173a",
   "metadata": {},
   "source": [
    "## Data Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a33267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss, Softmax\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b513bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Models/SmolLM2-360M-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                    load_in_4bit= True,\n",
    "                                    bnb_4bit_quant_type= \"nf4\",\n",
    "                                    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                    bnb_4bit_use_double_quant= True,\n",
    "                                )\n",
    "                                \n",
    "lora_r = 8\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLTechniques:\n",
    "    \"\"\"Container for all continual learning techniques\"\"\"\n",
    "\n",
    "    def __init__(self, model, device, technique=\"none\",\n",
    "                ewc_lambda=1000,\n",
    "                mem_size=100,\n",
    "                lwf_lambda=1,\n",
    "                temperature=2,\n",
    "                mas_lambda=1000):\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.technique = technique.lower()\n",
    "\n",
    "        # Initialize selected technique\n",
    "        if self.technique == \"ewc\":\n",
    "            self._init_ewc(ewc_lambda)\n",
    "        elif self.technique == \"agem\":\n",
    "            self._init_agem(mem_size)\n",
    "        elif self.technique == \"lwf\":\n",
    "            self._init_lwf(lwf_lambda, temperature)\n",
    "        elif self.technique == \"mas\":\n",
    "            self._init_mas(mas_lambda)\n",
    "\n",
    "    def _init_ewc(self, ewc_lambda):\n",
    "        \"\"\"Elastic Weight Consolidation\"\"\"\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.params = {n: p.clone().detach()\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if p.requires_grad}\n",
    "        self.fisher = {n: torch.zeros_like(p)\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if p.requires_grad}\n",
    "\n",
    "    def _init_agem(self, mem_size):\n",
    "        \"\"\"Average Gradient Episodic Memory\"\"\"\n",
    "        self.mem_size = mem_size\n",
    "        self.memory = []\n",
    "\n",
    "    def _init_lwf(self, lwf_lambda, temperature):\n",
    "        \"\"\"Learning Without Forgetting\"\"\"\n",
    "        self.lwf_lambda = lwf_lambda\n",
    "        self.temperature = temperature\n",
    "        self.old_model = None\n",
    "\n",
    "    def _init_mas(self, mas_lambda):\n",
    "        \"\"\"Memory Aware Synapses\"\"\"\n",
    "        self.mas_lambda = mas_lambda\n",
    "        self.importance = {n: torch.zeros_like(p)\n",
    "                        for n, p in self.model.named_parameters()\n",
    "                        if p.requires_grad}\n",
    "        self.old_params = deepcopy(self.importance)\n",
    "\n",
    "    def compute_regularization(self, inputs=None):\n",
    "        \"\"\"Compute CL regularization term\"\"\"\n",
    "        if self.technique == \"ewc\":\n",
    "            penalty = 0\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    penalty += (self.fisher[n] * (p - self.params[n]).pow(2)).sum()\n",
    "            return self.ewc_lambda * penalty\n",
    "\n",
    "        elif self.technique == \"lwf\" and self.old_model:\n",
    "            with torch.no_grad():\n",
    "                logits = inputs['logits']\n",
    "                actual_inputs = {k:torch.squeeze(v).to(self.device) for k,v in inputs.items() if k != \"logits\"}\n",
    "                old_outputs = self.old_model(**actual_inputs)\n",
    "            return self.lwf_lambda * KLDivLoss(reduction='batchmean')(\n",
    "                torch.log_softmax(logits/self.temperature, dim=1),\n",
    "                torch.softmax(old_outputs.logits/self.temperature, dim=1)\n",
    "            ) * (self.temperature ** 2)\n",
    "\n",
    "        elif self.technique == \"mas\":\n",
    "            penalty = 0\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    penalty += (self.importance[n] * (p - self.old_params[n]).pow(2)).sum()\n",
    "            return self.mas_lambda * penalty\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def pre_backward(self, inputs=None):\n",
    "        \"\"\"Operations before backward pass\"\"\"\n",
    "        if self.technique == \"agem\" and self.memory:\n",
    "            # Store current gradient\n",
    "            self.model.zero_grad()\n",
    "            for inputs_mem, labels_mem in self.memory:\n",
    "                inputs_mem = {k:torch.squeeze(v).to(self.device) for k,v in inputs_mem.items()}\n",
    "                labels_mem = torch.squeeze(labels_mem).to(self.device)\n",
    "                outputs = self.model(**inputs_mem)\n",
    "                loss = loss_f(outputs.logits, labels_mem)\n",
    "                loss.backward()\n",
    "\n",
    "            self.ref_grad = [p.grad.clone() for p in self.model.parameters() if p.requires_grad] # i think this should be with req grad\n",
    "            self.model.zero_grad()\n",
    "\n",
    "    def post_backward(self):\n",
    "        \"\"\"Operations after backward pass\"\"\"\n",
    "        if self.technique == \"agem\" and hasattr(self, 'ref_grad'):\n",
    "            # Project gradients\n",
    "            dot_product = sum(torch.sum(p.grad * g_ref)\n",
    "                        for p, g_ref in zip(self.model.parameters(), self.ref_grad) if p.requires_grad)\n",
    "            ref_norm = sum(torch.sum(g_ref * g_ref) for g_ref in self.ref_grad)\n",
    "\n",
    "            if dot_product < 0:  # Negative interference\n",
    "                scale = dot_product / (ref_norm + 1e-8)\n",
    "                for p, g_ref in zip(self.model.parameters(), self.ref_grad):\n",
    "                    if p.grad is not None and p.requires_grad:\n",
    "                        p.grad -= scale * g_ref\n",
    "\n",
    "    def post_task_update(self, dataloader=None):\n",
    "        \"\"\"Update after each task\"\"\"\n",
    "        if self.technique == \"ewc\":\n",
    "            # Compute Fisher information\n",
    "            self.model.eval()\n",
    "            for batch in dataloader:\n",
    "                self.model.zero_grad()\n",
    "                inputs = {k:torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                # outputs = self.model(**inputs, labels=labels)\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "                loss = loss_f(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        self.fisher[n] += p.grad.pow(2) / len(dataloader)\n",
    "\n",
    "            # Update stored parameters\n",
    "            self.params = {n: p.clone().detach()\n",
    "                        for n, p in self.model.named_parameters()\n",
    "                        if p.requires_grad}\n",
    "\n",
    "        elif self.technique == \"agem\":\n",
    "            # Update memory buffer\n",
    "            self.memory = []\n",
    "            for batch in dataloader:\n",
    "                inputs = {k: torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "                labels = torch.squeeze(batch['labels']).to(self.device)\n",
    "                self.memory.append((inputs, labels))\n",
    "                if len(self.memory) >= self.mem_size:\n",
    "                    break\n",
    "\n",
    "        elif self.technique == \"lwf\":\n",
    "            # Save model snapshot\n",
    "            self.old_model = deepcopy(self.model)\n",
    "            self.old_model.eval()\n",
    "\n",
    "        elif self.technique == \"mas\":\n",
    "            # Update importance weights\n",
    "            self.model.eval()\n",
    "            for batch in dataloader:\n",
    "                self.model.zero_grad()\n",
    "                inputs = {k: torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                # does this need a loss?????????????\n",
    "                torch.norm(outputs.logits, p=2, dim=1).mean().backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        self.importance[n] += p.grad.abs() / len(dataloader)\n",
    "\n",
    "            # Update stored parameters\n",
    "            self.old_params = {n: p.clone().detach()\n",
    "                            for n, p in self.model.named_parameters()\n",
    "                            if p.requires_grad}\n",
    "\n",
    "class AutoContinualLearner(nn.Module):\n",
    "    def __init__(self, model_name, device, quantization_config, torch_dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch_dtype\n",
    "        )\n",
    "        self.n_initial_params = sum(t.numel() for t in self.model.parameters())\n",
    "        self.n_trainable_params_initial = sum(t.numel() for t in self.model.parameters() if t.requires_grad)\n",
    "        self.cl = None\n",
    "\n",
    "    def init_cl(self, technique, lora_config, **kwargs):\n",
    "        \"\"\"Init the continual learning technique\"\"\"\n",
    "        self.model = get_peft_model(self.model, lora_config).to(self.device)\n",
    "        self.n_params_lora = sum(t.numel() for t in self.model.parameters())\n",
    "        self.n_trainable_params_lora = sum(t.numel() for t in self.model.parameters() if t.requires_grad)\n",
    "        self.model.print_trainable_parameters()\n",
    "        self.cl = CLTechniques(self.model, self.device, technique, **kwargs)\n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041aca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_technique = \"ewc\"\n",
    "\n",
    "if cl_technique in [\"ewc\", \"agem\", \"lwf\", \"mas\"]:\n",
    "    cl_hyperparams = {\n",
    "    \"ewc\": {\"ewc_lambda\":1500},\n",
    "    \"agem\": {\"mem_size\":100},\n",
    "    \"lwf\": {\"lwf_lambda\":1,\n",
    "            \"temperature\":2},\n",
    "    \"mas\": {\"mas_lambda\":1000}\n",
    "    }\n",
    "\n",
    "    cl_params = cl_hyperparams[cl_technique]\n",
    "    hyper_param_str = \"=\".join([str(k) + \"-\" + str(v) for k, v in cl_params.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27232141",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,638,400 || all params: 363,459,520 || trainable%: 0.4508\n"
     ]
    }
   ],
   "source": [
    "model = AutoContinualLearner(model_id + \"/Model\", device, bnb_config)\n",
    "model.init_cl(technique=cl_technique, lora_config=config, **cl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id + \"/Tokenizer\")\n",
    "if tokenizer.pad_token is None and \"Llama\" in model_id: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "elif tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = open(model_id + \"/Tokenizer/chat_template.jinja\").read()\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=512):\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # for the cross entropy loss\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f15e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"df_from_exp_to_imp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848352f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Preparing the Data-----------------\n",
      "_________________________________\n",
      "Loading and filtering the Data\n"
     ]
    }
   ],
   "source": [
    "print(\"----------Preparing the Data-----------------\")\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading and filtering the Data\")\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "testing_order = [\"explicit_hs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d6bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task\n",
      "explicit_hs\n"
     ]
    }
   ],
   "source": [
    "datasets_test = []\n",
    "for i, task in enumerate(testing_order):\n",
    "    print(\"Task\")\n",
    "    print(task)\n",
    "    split_df = df[(df[\"split\"] == \"test\") & (df[\"task\"] == task)]\n",
    "    hf_split = Dataset.from_pandas(split_df)\n",
    "    datasets_test.append({testing_order[i]: hf_split})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91ce08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'explicit_hs': Dataset({\n",
       "      features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
       "      num_rows: 720\n",
       "  })}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a778f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = datasets_test[0][\"explicit_hs\"][21][\"formatted_prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are a social media content moderator.\\n'\n",
      " 'INSTRUCTION: The following is a social media message that needs to be '\n",
      " 'classified with the label HATEFUL or NOT HATEFUL.\\n'\n",
      " \"MESSAGE: does this mean white south africans aren't actually white  because \"\n",
      " \"they're born in pri\\n\"\n",
      " 'OUTPUT AND FORMAT: your output should be just the label.')\n"
     ]
    }
   ],
   "source": [
    "pp(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": example}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b5abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|im_start|>system\\n'\n",
      " 'You are a helpful assistant<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'You are a social media content moderator.\\n'\n",
      " 'INSTRUCTION: The following is a social media message that needs to be '\n",
      " 'classified with the label HATEFUL or NOT HATEFUL.\\n'\n",
      " \"MESSAGE: does this mean white south africans aren't actually white  because \"\n",
      " \"they're born in pri\\n\"\n",
      " 'OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n')\n"
     ]
    }
   ],
   "source": [
    "chat_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "pp(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750fa788",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
    "input_ids_tokenized = input_dict[\"input_ids\"]\n",
    "attention_mask = input_dict[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c259e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = model.model.generate(input_ids=input_ids_tokenized, \n",
    "                                        attention_mask=attention_mask, \n",
    "                                        top_p=0.9, \n",
    "                                        temperature=0.6, \n",
    "                                        max_new_tokens=10,\n",
    "                                        return_dict_in_generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cf151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
       "            1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
       "           30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
       "         1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
       "         4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
       "         6565,    30,   198, 26826, 41601,    42,  1072,   451,  1441,  2537,\n",
       "         4203, 46230,   487,  4775,   982,  2390,  2537,   216,   975,   502,\n",
       "         2316,  3988,   281, 24993,   198, 44724,  7485, 12111, 40560,    42,\n",
       "          469,  3124,   868,   325,   915,   260,  4368,    30,     2,   198,\n",
       "            1,   520,  9531,   198, 18083,   407,  9081,    54,  6565,     2],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = generated_tokens[0]\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tokenizer.decode(seq[input_ids_tokenized.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2797d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NOT HATEFUL'\n"
     ]
    }
   ],
   "source": [
    "pp(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80944123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Testing the model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# prompt_plus_messages = base_prompt.format(clean_post)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# print(\"FORMATTED PROMPT\")\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# print(formatted_prompt)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     38\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     39\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: formatted_prompt}\n\u001b[0;32m     40\u001b[0m ]\n\u001b[1;32m---> 41\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# print(\"CHAT TEMPLATE COMPUTED\")\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# print(chat_template)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m tokenizer(chat_template, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1652\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1649\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1651\u001b[0m template_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[1;32m-> 1652\u001b[0m rendered_chat, generation_indices \u001b[38;5;241m=\u001b[39m \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[0;32m   1664\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m rendered_chat[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:498\u001b[0m, in \u001b[0;36mrender_jinja_template\u001b[1;34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 498\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[0;32m    506\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jinja2\\environment.py:1295\u001b[0m, in \u001b[0;36mTemplate.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jinja2\\environment.py:942\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[1;32m--> 942\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[1;32m<template>:23\u001b[0m, in \u001b[0;36mtop-level template code\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "\n",
    "predictions_test = []\n",
    "labels_test = []\n",
    "predicted_strings = []\n",
    "labels_strings = []\n",
    "full_generation = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # print(\"TESTING DS\")\n",
    "    # print(ds)\n",
    "    # print()\n",
    "    # for i, test_item in enumerate(ds[\"test\"]):\n",
    "    for i, test_item in enumerate(datasets_test[0].values()):\n",
    "        # print(\"TESTING ITEM\")\n",
    "        # print(test_item)\n",
    "        # print(i)\n",
    "        target_label = test_item[\"label\"]\n",
    "        labels_strings.append(target_label)\n",
    "        # print(\"TARGET LABEL\")\n",
    "        # print(target_label)\n",
    "        if target_label == \"NOT HATEFUL\":\n",
    "            target_label = 0\n",
    "        elif target_label == \"HATEFUL\":\n",
    "            target_label = 1\n",
    "        \n",
    "        labels_test.append(target_label)\n",
    "\n",
    "        formatted_prompt = test_item[\"formatted_prompt\"]\n",
    "        # prompt_plus_messages = base_prompt.format(clean_post)\n",
    "        # print(\"FORMATTED PROMPT\")\n",
    "        # print(formatted_prompt)\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "        chat_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        # print(\"CHAT TEMPLATE COMPUTED\")\n",
    "        # print(chat_template)\n",
    "        input_dict = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
    "        input_ids_tokenized = input_dict[\"input_ids\"]\n",
    "        attention_mask = input_dict[\"attention_mask\"]\n",
    "        \n",
    "        # print(\"TOKENIZED CHAT TEMPLATE COMPUTED\")\n",
    "        # print(input_ids_tokenized)\n",
    "        # print(type(input_ids_tokenized))\n",
    "        # if input_ids_tokenized.shape[0] == 1:\n",
    "        #     print(\"wrong size\")\n",
    "        #     input_ids_tokenized = input_ids_tokenized.squeeze(0)\n",
    "        #     attention_mask = attention_mask.squeeze(0)\n",
    "        # print(\"NEW SHAPE\")\n",
    "        # print(input_ids_tokenized.shape)\n",
    "        # print(attention_mask.shape)\n",
    "        # ######################\n",
    "        # print(\"----------------right beforeoutput---------------------------------------\")\n",
    "        # # print(model)\n",
    "        # # print(model.module)\n",
    "        # # print(dir(model))\n",
    "        # # print(dir(model.module))\n",
    "        # # print(help(model.module.generate))\n",
    "        # print(model.module.generate(input_ids=input_ids_tokenized, \n",
    "        #                                 attention_mask=attention_mask, \n",
    "        #                                 top_p=0.9, \n",
    "        #                                 temperature=0.6, \n",
    "        #                                 max_new_tokens=10,\n",
    "        #                                 return_dict_in_generate=False))\n",
    "        # print(\"----------------right after output---------------------------------------\")\n",
    "        output = model.module.model.generate(input_ids=input_ids_tokenized, \n",
    "                                        attention_mask=attention_mask, \n",
    "                                        top_p=0.9, \n",
    "                                        temperature=0.6, \n",
    "                                        max_new_tokens=10,\n",
    "                                        return_dict_in_generate=False)\n",
    "                \n",
    "        # pred = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        # print(\"OUTPUT COMPUTED\")\n",
    "        # print(output)\n",
    "        # print(type(output))\n",
    "        seq = output[0]\n",
    "        # print(tokenizer.decode(seq, skip_special_tokens=True).strip())\n",
    "        pred = tokenizer.decode(seq[input_ids_tokenized.shape[1]:], skip_special_tokens=True)\n",
    "        print(pred)\n",
    "        print(seq)\n",
    "        full_generation.append(pred)\n",
    "        predicted_strings.append(pred)\n",
    "        # print(\"PRED COMPUTED\")\n",
    "        # print(pred)\n",
    "        pred_label = translate_prediction_to_label(pred)\n",
    "        # print(\"PRED LABEL COMPUTED\")\n",
    "        # print(pred_label)\n",
    "        predictions_test.append(pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3dbb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:750: UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     local_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m world_size \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------Preparing the Data-----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_________________________________\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:2330\u001b[0m, in \u001b[0;36mget_world_size\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[0;32m   2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1096\u001b[0m, in \u001b[0;36m_get_group_size\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a given group's world size.\"\"\"\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1302\u001b[0m, in \u001b[0;36m_get_default_group\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[1;32m-> 1302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember\u001b[38;5;241m.\u001b[39mWORLD)\n",
      "\u001b[1;31mValueError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "\n",
    "times_array = list(df[\"time\"].unique())\n",
    "datasets = []\n",
    "dataset_names = list(df[\"task\"].unique())\n",
    "\n",
    "for time in times_array:\n",
    "\n",
    "    time_ds = []\n",
    "    for split in df[\"split\"].unique():\n",
    "\n",
    "        split_df = df[(df[\"split\"] == split) & (df[\"time\"] == time)]\n",
    "        hf_split = Dataset.from_pandas(split_df)\n",
    "        time_ds.append(hf_split)\n",
    "    datasets.append(time_ds)\n",
    "\n",
    "hf_datasets = []\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "\n",
    "    hf_ds = DatasetDict({dataset[0][\"split\"][0]: dataset[0], \n",
    "                        dataset[1][\"split\"][0]: dataset[1],\n",
    "                        dataset[2][\"split\"][0]: dataset[2]})\n",
    "    hf_ds_name = dataset_names[i]\n",
    "    hf_datasets.append({hf_ds_name: hf_ds})\n",
    "\n",
    "hf_datasets = [\n",
    "    {task_name: hf_time.map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items() \n",
    "]\n",
    "\n",
    "n_samples_per_ds = [\n",
    "    len(hf_time[\"train\"])\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items() \n",
    "]\n",
    "\n",
    "for ds in hf_datasets:\n",
    "    for hf_data in ds.values():\n",
    "        hf_data.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \n",
    "                \"target\", \"implied_statement\", \"split\", \"time\", \"task\",\n",
    "                \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "hf_datasets = [\n",
    "    {task_name: {split: hf_time[split].remove_columns(cols_to_remove)}}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items()\n",
    "    for split in hf_time \n",
    "    if split != \"test\"]\n",
    "\n",
    "print(\"hf_datasets before data collator:\")\n",
    "print(hf_datasets)\n",
    "print()\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# distributed_samplers = [\n",
    "#     {task_name: {split: DistributedSampler(hf_time[split], num_replicas=world_size, rank=local_rank, shuffle=False)}}\n",
    "#     for hf_data in hf_datasets\n",
    "#     for task_name, hf_time in hf_data.items()\n",
    "#     for split in hf_time \n",
    "#     if split != \"test\"\n",
    "# ]\n",
    "\n",
    "distributed_samplers = []\n",
    "for ds in hf_datasets:\n",
    "    ds_dict = {}\n",
    "    print(\"ds:\")\n",
    "    print(ds)\n",
    "    for task_name, hf_data in ds.items():\n",
    "        print(\"task_name:\")\n",
    "        print(task_name)\n",
    "        print(\"hf_data:\")\n",
    "        print(hf_data)\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            print(\"split:\")\n",
    "            print(split)\n",
    "            if split != \"test\":\n",
    "                distr_sampler = DistributedSampler(hf_data[split], num_replicas=world_size, rank=local_rank, shuffle=False)\n",
    "                ds_dict[task_name][split] = distr_sampler\n",
    "        dsitr_samplers.append(ds_dict)\n",
    "\n",
    "data_loaders = []\n",
    "for i, distr_sampler in enumerate(distributed_samplers):\n",
    "    ds_name = list(distr_sampler.keys())[0]\n",
    "    ds_dict = {}\n",
    "    ds_dict[ds_name] = {}\n",
    "    for split, distributed_sampler in distr_sampler[ds_name].items():\n",
    "        data_loader = DataLoader(hf_datasets[i][ds_name][split], collate_fn=data_collator, batch_size=batch_size, sampler=distributed_sampler)\n",
    "        ds_dict[ds_name][split] = data_loader\n",
    "    data_loaders.append(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd59cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "distributed_samplers = [\n",
    "    {task_name: {split: DistributedSampler(hf_time[split], num_replicas=world_size, rank=local_rank, shuffle=False)}}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items()\n",
    "    for split in hf_time \n",
    "    if split != \"test\"\n",
    "]\n",
    "\n",
    "data_loaders = []\n",
    "for i, distr_sampler in enumerate(distributed_samplers):\n",
    "    ds_name = list(distr_sampler.keys())[0]\n",
    "    ds_dict = {}\n",
    "    ds_dict[ds_name] = {}\n",
    "    for split, distributed_sampler in distr_sampler[ds_name].items():\n",
    "        data_loader = DataLoader(hf_datasets[i][ds_name][split], collate_fn=data_collator, batch_size=batch_size, sampler=distributed_sampler)\n",
    "        ds_dict[ds_name][split] = data_loader\n",
    "    data_loaders.append(ds_dict)\n",
    "\n",
    "# data loader = []\n",
    "# each item in the list is a dictionary of {<dataset_name>: {<split>: <dataloade>}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc206b0",
   "metadata": {},
   "source": [
    "## Model Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97446dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fe56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Models/Llama-3.2-1B-Instruct/Model\"\n",
    "tokenizer_id = \"Models/Llama-3.2-1B-Instruct/Tokenizer\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# model.save_pretrained(\"Models/Llama-3.2-1B-Instruct/Model\")\n",
    "# tokenizer.save_pretrained(\"Models/Llama-3.2-1B-Instruct/Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") \n",
    "# log_hf()\n",
    "load_dotenv(\"env_vars.env\")\n",
    "\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_epochs = 2\n",
    "lr = 1e-5\n",
    "lora_r = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546388f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Preapring the Data\n"
     ]
    }
   ],
   "source": [
    "########################################################## DATA WORK\n",
    "print(\"_________________________________\")\n",
    "print(\"Preapring the Data\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"df_from_exp_to_imp.csv\")\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09a2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66202d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abdf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = '<|finetune_right_pad_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d07937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|finetune_right_pad_id|>\n",
      "list[0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(type(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b884688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.pad_token, add_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab66f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafd5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "print(tokens.shape)\n",
    "second = tokens.fill_(-100)\n",
    "tok = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# torch_tensor = torch.tensor(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e14b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100, 128000,   9906,\n",
       "             11,   1268,    527,    499,     30]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both = torch.cat((tokens,tok), dim=1)\n",
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cd57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(both)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e711b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|finetune_right_pad_id|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=312):\n",
    "\n",
    "    # if type(label) != list:\n",
    "    #     label = [label]\n",
    "    # if type(clean_post) != list:\n",
    "    #     clean_post = [clean_post]\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    # pp(prompt_plus_messages)\n",
    "    # pp(label)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    # print(messages)\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    # print(chat_template)\n",
    "\n",
    "    # why is the chat template putting a new line at the end of the end of sequence\n",
    "    # pp(chat_template)\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "    # print(input_ids_tokenized)\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # acc to llama docs\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    print(\"FILLING PAD WITH\")\n",
    "    print(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    eos_n_tensor.fill_(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(logits, labels):\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "t_1 = []\n",
    "t_2 = []\n",
    "\n",
    "for split in df[\"split\"].unique():\n",
    "\n",
    "    split_df_1 = df[(df[\"split\"] == split) & (df[\"time\"] == 1)]\n",
    "    split_df_2 = df[(df[\"split\"] == split) & (df[\"time\"] == 2)]\n",
    "\n",
    "    hf_split_1 = Dataset.from_pandas(split_df_1)\n",
    "    hf_split_2 = Dataset.from_pandas(split_df_2)\n",
    "    \n",
    "    t_1.append(hf_split_1)\n",
    "    t_2.append(hf_split_2)\n",
    "\n",
    "hf_time_1 = DatasetDict({t_1[0][\"split\"][0]: t_1[0], \n",
    "                        t_1[1][\"split\"][0]: t_1[1],\n",
    "                        t_1[2][\"split\"][0]: t_1[2]})\n",
    "\n",
    "hf_time_2 = DatasetDict({t_2[0][\"split\"][0]: t_2[0], \n",
    "                        t_2[1][\"split\"][0]: t_2[1],\n",
    "                        t_2[2][\"split\"][0]: t_2[2]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = hf_time_1[\"train\"][0]\n",
    "input_model = preprocess_and_tokenize(ex[\"clean_post\"], ex[\"label\"], base_prompt=base_prompt, max_length=312)\n",
    "input_model[\"labels\"]\n",
    "input_model[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = model(**input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd14464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3625, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c4d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 311, 128256])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a638b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 311])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_model[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01330643",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output_model.logits\n",
    "labels = input_model[\"labels\"]\n",
    "\n",
    "flat_logits = logits.view(-1, logits.size(-1))\n",
    "flat_labels = labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523653e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311, 128256])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eaaf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af1681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9375, dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss(ignore_index=-100)  \n",
    "loss = loss_fn(flat_logits, flat_labels)          \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dd8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7edf5",
   "metadata": {},
   "source": [
    "Checking that the -100 is not being computed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32272c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (flat_labels != -100)\n",
    "mask.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b55fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_logits = flat_logits[mask]   \n",
    "valid_labels = flat_labels[mask]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12140e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9375, dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_loss = loss_fn(valid_logits, valid_labels)\n",
    "manual_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(logits, labels):\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 1300.52 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:04<00:00, 1307.97 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 1364.92 examples/s]\n",
      "Map: 100%|██████████| 1019/1019 [00:00<00:00, 1268.26 examples/s]\n",
      "Map: 100%|██████████| 8155/8155 [00:06<00:00, 1311.17 examples/s]\n",
      "Map: 100%|██████████| 1020/1020 [00:00<00:00, 1156.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "########################################################## TOKENIZER WORK\n",
    "\n",
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "hf_time_2 = hf_time_2.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaec1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT HATEFUL'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_time_1[\"train\"][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79750cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(151643)\n",
    "end_of_text_token = 151643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_encoding = tokenizer.encode(\"HATEFUL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a221fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hate_encoding = tokenizer.encode(\"NOT HATEFUL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcef0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'formatted_prompt', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = hf_time_1[\"train\"][0]\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135abd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_prompt = example[\"input_ids\"][0].index(end_of_text_token)\n",
    "end_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14065, 472, 2336, 49636, 151643]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoding = tokenizer.encode(example[\"label\"] + tokenizer.eos_token)\n",
    "label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226d8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(14065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_answer = end_prompt-(len(label_encoding)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0][: start_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81880492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT HATEFUL<|im_end|>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0][start_answer : end_prompt - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'formatted_prompt', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01060ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdb98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0739894",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=312):\n",
    "\n",
    "    # if type(label) != list:\n",
    "    #     label = [label]\n",
    "    # if type(clean_post) != list:\n",
    "    #     clean_post = [clean_post]\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    # pp(prompt_plus_messages)\n",
    "    # pp(label)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    # print(messages)\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    # print(chat_template)\n",
    "\n",
    "    # why is the chat template putting a new line at the end of the end of sequence\n",
    "    # pp(chat_template)\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "    # print(input_ids_tokenized)\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # acc to llama docs\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.eos_token_id)\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171650b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 821.43 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:07<00:00, 818.96 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:01<00:00, 622.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_time_1.set_format(\"torch\")\n",
    "hf_time_2.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \"target\", \"implied_statement\", \"split\", \"time\", \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "for split in hf_time_1:\n",
    "    if split != \"test\":\n",
    "        hf_time_1[split] = hf_time_1[split].remove_columns(cols_to_remove)\n",
    "        hf_time_2[split] = hf_time_2[split].remove_columns(cols_to_remove)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "hf_time_1_train_loader = DataLoader(hf_time_1[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_validation_loader = DataLoader(hf_time_1[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_test_loader = DataLoader(hf_time_1[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "hf_time_2_train_loader = DataLoader(hf_time_2[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_validation_loader = DataLoader(hf_time_2[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_test_loader = DataLoader(hf_time_2[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "# ### So far, created the prompt, did the messages with the prompt and answer in place. Applied to chat template and tokenized \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Loading the model and model config\n",
      "Model Size before LoRA 315119488\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "\n",
      "Model After LoRA\n",
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "########################3#################### MODEL WORK\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading the model and model config\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\",\n",
    "                                            quantization_config=bnb_config\n",
    "                                            )\n",
    "\n",
    "# to deal with the fact that we dont make the first token prediction??\n",
    "\n",
    "\n",
    "model_size_before = sum(t.numel() for t in model.parameters())\n",
    "print(\"Model Size before LoRA\", model_size_before)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print(\"Model After LoRA\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Training the model\")\n",
    "print()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.train()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(hf_time_1_train_loader):\n",
    "        if i > 0:\n",
    "            continue\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"\\tBatch: \", i)\n",
    "        # print(batch)\n",
    "        batch.to(device)\n",
    "        # print(batch.keys())\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        output = model(**batch)\n",
    "        logits = output.logits\n",
    "        loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "        print(batch.keys())\n",
    "        print(loss.detach().item())\n",
    "        print(output.logits.shape)\n",
    "        print(output.probas)\n",
    "\n",
    "        if i > 3:\n",
    "            continue\n",
    "\n",
    "    epoch_loss = sum(losses)/len(hf_time_1_train_loader)\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        for i, batch in enumerate(hf_time_1_validation_loader):\n",
    "            if i > 0:\n",
    "                continue\n",
    "            batch.to(device)\n",
    "            batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "            logits = output.logits\n",
    "            val_loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "            val_losses.append(val_loss.detach().item())\n",
    "\n",
    "        val_loss_epoch = sum(val_losses)/len(hf_time_1_validation_loader)\n",
    "        print(f\"Epoch {epoch} Validation Loss: {val_loss_epoch}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "for i, test_batch in enumerate(hf_time_1[\"test\"]):\n",
    "\n",
    "    if i > 0:\n",
    "        break\n",
    "    \n",
    "    text = test_batch[\"formatted_prompt\"]\n",
    "    tokenized_chat_template, messages_list = preprocess_and_tokenize(text, label=False, add_generation_prompt=True, output_messages_list=True)\n",
    "    output = model.generate(**tokenized_chat_template.to(device))\n",
    "    pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(text)\n",
    "    print(tokenized_chat_template)\n",
    "    print(output)\n",
    "    print(pred)\n",
    "\n",
    "print(\"CHECKING GENERATION\")\n",
    "print(messages_list)\n",
    "\n",
    "print(tokenized_chat_template)\n",
    "print(output)\n",
    "\n",
    "print(type(output))\n",
    "print(output.shape)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Saving the model and Tokenizer\")\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "model.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "tokenizer.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "\n",
    "print(\"RUN SUCCESSFULLY\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1e57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import sys \n",
    "import warnings \n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import whoami, HfFolder\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3e640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1deefbe",
   "metadata": {},
   "source": [
    "## Checking whats wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17cd9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import sys \n",
    "import warnings \n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "from dotenv import load_dotenv\n",
    "from datetime import date\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import emoji\n",
    "import json\n",
    "from huggingface_hub import whoami, HfFolder\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss, Softmax, KLDivLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432a4591",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llm_experiments_set_up.json\", \"r\") as f:\n",
    "    exp_setup = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2645a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=None\n",
    "batch_size=16\n",
    "type_experiment=\"from_expl_to_impl\"\n",
    "cl_technique=\"ewc\"\n",
    "model_id = \"Models/SmolLM2-360M-Instruct\"\n",
    "training_order=[\"explicit_hs\", \"implicit_hs\"]\n",
    "testing_order=[\"explicit_hs\", \"implicit_hs\"]\n",
    "batch_size = batch_size\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "lora_r = 8\n",
    "exp_setup = exp_setup\n",
    "mode = None\n",
    "dataset_path=\"df_from_exp_to_imp.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161578e",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646a7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLTechniques:\n",
    "    \"\"\"Container for all continual learning techniques\"\"\"\n",
    "\n",
    "    def __init__(self, model, device, technique=\"none\",\n",
    "                ewc_lambda=1000,\n",
    "                mem_size=100,\n",
    "                lwf_lambda=1,\n",
    "                temperature=2,\n",
    "                mas_lambda=1000):\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.technique = technique.lower()\n",
    "\n",
    "        # Initialize selected technique\n",
    "        if self.technique == \"ewc\":\n",
    "            self._init_ewc(ewc_lambda)\n",
    "        elif self.technique == \"agem\":\n",
    "            self._init_agem(mem_size)\n",
    "        elif self.technique == \"lwf\":\n",
    "            self._init_lwf(lwf_lambda, temperature)\n",
    "        elif self.technique == \"mas\":\n",
    "            self._init_mas(mas_lambda)\n",
    "\n",
    "    def _init_ewc(self, ewc_lambda):\n",
    "        \"\"\"Elastic Weight Consolidation\"\"\"\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.params = {n: p.clone().detach()\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if p.requires_grad}\n",
    "        self.fisher = {n: torch.zeros_like(p)\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if p.requires_grad}\n",
    "\n",
    "    def _init_agem(self, mem_size):\n",
    "        \"\"\"Average Gradient Episodic Memory\"\"\"\n",
    "        self.mem_size = mem_size\n",
    "        self.memory = []\n",
    "\n",
    "    def _init_lwf(self, lwf_lambda, temperature):\n",
    "        \"\"\"Learning Without Forgetting\"\"\"\n",
    "        self.lwf_lambda = lwf_lambda\n",
    "        self.temperature = temperature\n",
    "        self.old_model = None\n",
    "\n",
    "    def _init_mas(self, mas_lambda):\n",
    "        \"\"\"Memory Aware Synapses\"\"\"\n",
    "        self.mas_lambda = mas_lambda\n",
    "        self.importance = {n: torch.zeros_like(p)\n",
    "                        for n, p in self.model.named_parameters()\n",
    "                        if p.requires_grad}\n",
    "        self.old_params = deepcopy(self.importance)\n",
    "\n",
    "    def compute_regularization(self, inputs=None):\n",
    "        \"\"\"Compute CL regularization term\"\"\"\n",
    "        if self.technique == \"ewc\":\n",
    "            penalty = 0\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    penalty += (self.fisher[n] * (p - self.params[n]).pow(2)).sum()\n",
    "            return self.ewc_lambda * penalty\n",
    "\n",
    "        elif self.technique == \"lwf\" and self.old_model:\n",
    "            with torch.no_grad():\n",
    "                logits = inputs['logits']\n",
    "                actual_inputs = {k:torch.squeeze(v).to(self.device) for k,v in inputs.items() if k != \"logits\"}\n",
    "                old_outputs = self.old_model(**actual_inputs)\n",
    "            return self.lwf_lambda * KLDivLoss(reduction='batchmean')(\n",
    "                torch.log_softmax(logits/self.temperature, dim=1),\n",
    "                torch.softmax(old_outputs.logits/self.temperature, dim=1)\n",
    "            ) * (self.temperature ** 2)\n",
    "\n",
    "        elif self.technique == \"mas\":\n",
    "            penalty = 0\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    penalty += (self.importance[n] * (p - self.old_params[n]).pow(2)).sum()\n",
    "            return self.mas_lambda * penalty\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def pre_backward(self, inputs=None):\n",
    "        \"\"\"Operations before backward pass\"\"\"\n",
    "        if self.technique == \"agem\" and self.memory:\n",
    "            # Store current gradient\n",
    "            self.model.zero_grad()\n",
    "            for inputs_mem, labels_mem in self.memory:\n",
    "                inputs_mem = {k:torch.squeeze(v).to(self.device) for k,v in inputs_mem.items()}\n",
    "                labels_mem = torch.squeeze(labels_mem).to(self.device)\n",
    "                outputs = self.model(**inputs_mem)\n",
    "                loss = loss_f(outputs.logits, labels_mem)\n",
    "                loss.backward()\n",
    "\n",
    "            self.ref_grad = [p.grad.clone() for p in self.model.parameters() if p.requires_grad] # i think this should be with req grad\n",
    "            self.model.zero_grad()\n",
    "\n",
    "    def post_backward(self):\n",
    "        \"\"\"Operations after backward pass\"\"\"\n",
    "        if self.technique == \"agem\" and hasattr(self, 'ref_grad'):\n",
    "            # Project gradients\n",
    "            dot_product = sum(torch.sum(p.grad * g_ref)\n",
    "                        for p, g_ref in zip(self.model.parameters(), self.ref_grad) if p.requires_grad)\n",
    "            ref_norm = sum(torch.sum(g_ref * g_ref) for g_ref in self.ref_grad)\n",
    "\n",
    "            if dot_product < 0:  # Negative interference\n",
    "                scale = dot_product / (ref_norm + 1e-8)\n",
    "                for p, g_ref in zip(self.model.parameters(), self.ref_grad):\n",
    "                    if p.grad is not None and p.requires_grad:\n",
    "                        p.grad -= scale * g_ref\n",
    "\n",
    "    def post_task_update(self, dataloader=None):\n",
    "        \"\"\"Update after each task\"\"\"\n",
    "        if self.technique == \"ewc\":\n",
    "            # Compute Fisher information\n",
    "            self.model.eval()\n",
    "            for batch in dataloader:\n",
    "                self.model.zero_grad()\n",
    "                inputs = {k:torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                # outputs = self.model(**inputs, labels=labels)\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "                loss = loss_f(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        self.fisher[n] += p.grad.pow(2) / len(dataloader)\n",
    "\n",
    "            # Update stored parameters\n",
    "            self.params = {n: p.clone().detach()\n",
    "                        for n, p in self.model.named_parameters()\n",
    "                        if p.requires_grad}\n",
    "\n",
    "        elif self.technique == \"agem\":\n",
    "            # Update memory buffer\n",
    "            self.memory = []\n",
    "            for batch in dataloader:\n",
    "                inputs = {k: torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "                labels = torch.squeeze(batch['labels']).to(self.device)\n",
    "                self.memory.append((inputs, labels))\n",
    "                if len(self.memory) >= self.mem_size:\n",
    "                    break\n",
    "\n",
    "        elif self.technique == \"lwf\":\n",
    "            # Save model snapshot\n",
    "            self.old_model = deepcopy(self.model)\n",
    "            self.old_model.eval()\n",
    "\n",
    "        elif self.technique == \"mas\":\n",
    "            # Update importance weights\n",
    "            self.model.eval()\n",
    "            for batch in dataloader:\n",
    "                self.model.zero_grad()\n",
    "                inputs = {k: torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                # does this need a loss?????????????\n",
    "                torch.norm(outputs.logits, p=2, dim=1).mean().backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        self.importance[n] += p.grad.abs() / len(dataloader)\n",
    "\n",
    "            # Update stored parameters\n",
    "            self.old_params = {n: p.clone().detach()\n",
    "                            for n, p in self.model.named_parameters()\n",
    "                            if p.requires_grad}\n",
    "\n",
    "class AutoContinualLearner(nn.Module):\n",
    "    def __init__(self, model_name, device, quantization_config, torch_dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch_dtype\n",
    "        )\n",
    "        self.n_initial_params = sum(t.numel() for t in self.model.parameters())\n",
    "        self.n_trainable_params_initial = sum(t.numel() for t in self.model.parameters() if t.requires_grad)\n",
    "        self.cl = None\n",
    "\n",
    "    def init_cl(self, technique, lora_config, **kwargs):\n",
    "        \"\"\"Init the continual learning technique\"\"\"\n",
    "        self.model = get_peft_model(self.model, lora_config).to(self.device)\n",
    "        self.n_params_lora = sum(t.numel() for t in self.model.parameters())\n",
    "        self.n_trainable_params_lora = sum(t.numel() for t in self.model.parameters() if t.requires_grad)\n",
    "        self.model.print_trainable_parameters()\n",
    "        self.cl = CLTechniques(self.model, self.device, technique, **kwargs)\n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f2759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_failed_batch(batch):\n",
    "    \n",
    "    print(\"FAILED UNSQUEEZED BATCH\")\n",
    "    if \"failed_batches.json\" not in list(os.listdir()):\n",
    "        with open(\"failed_batches.json\", \"w\") as f:\n",
    "            json.dump([], f)\n",
    "    with open(\"failed_batches.json\", \"r\") as f:\n",
    "        failed_batches = json.load(f)\n",
    "    failed_batches.append(batch)\n",
    "    with open(\"failed_batches.json\", \"w\") as f:\n",
    "        json.dump(failed_batches, f)\n",
    "\n",
    "def log_hf():\n",
    "    \n",
    "    load_dotenv(\"env_vars.env\")\n",
    "    hf_token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    HfFolder.save_token(hf_token)\n",
    "    return print(whoami()[\"name\"])\n",
    "\n",
    "def setup():\n",
    "    try:\n",
    "        dist.init_process_group(\"nccl\")\n",
    "        # dist.init_process_group(\"gloo\")\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        return local_rank\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"DISTR TRAINING ALREADY INITIALIZED\")\n",
    "        local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        return local_rank\n",
    "\n",
    "\n",
    "def save_results_csv(df, experiment_name, model_id, cl_technique, result_type=\"specific\"):\n",
    "\n",
    "    cl_technique_clean = cl_technique.replace(\" + \", \"__\")\n",
    "    id_ = model_id.replace(\"/\", \"-\") + \"_\" + cl_technique_clean + \"_\" + str(date.today())\n",
    "    id_clean = experiment_name + id_.replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\",\"-\") + result_type + \".csv\"\n",
    "    df.to_csv(id_clean, index=False)\n",
    "    return print(\"Saved in path: \", id_clean)\n",
    "\n",
    "def clean_cl_name(cl_name):\n",
    "\n",
    "    regex = r'<(?:[\\w\\.]+)?\\.([\\w]+) object at'\n",
    "    matches =   re.findall(regex, cl_name)\n",
    "    clean_string = \" + \".join(matches)\n",
    "    return clean_string\n",
    "\n",
    "def clean_metric_name(metric_name):\n",
    "\n",
    "    reg = r\"\\s([a-z_1]+)\\s\"\n",
    "    match_ = re.search(reg, metric_name)\n",
    "    clean_str = match_.group().strip()\n",
    "\n",
    "    return clean_str\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n",
    "\n",
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def get_probability_distribution(logits):\n",
    "    probability_dist = Softmax(dim=-1)(logits)\n",
    "    return probability_dist\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100) # ignore the left pad tokens\n",
    "def loss_f(logits, labels):\n",
    "\n",
    "    flat_logits = logits.view(-1, logits.size(-1))\n",
    "    flat_labels = labels.view(-1)\n",
    "\n",
    "    loss = loss_fn(flat_logits, flat_labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def translate_prediction_to_label(text):\n",
    "    if \"NOT HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    elif \"NOT_HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT_HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else: \n",
    "            return 0\n",
    "    elif \"HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"HATEFUL\", \"\")\n",
    "        if \"NOT_HATEFUL\" in text_clean or \"NOT HATEFUL\" in text_clean:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891c086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Preapring the Tokenizer\n",
      "----------Preparing the Data-----------------\n",
      "_________________________________\n",
      "Loading and filtering the Data\n",
      "dataset\n",
      "dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 872.14 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:06<00:00, 855.06 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 841.59 examples/s]\n",
      "Map: 100%|██████████| 1019/1019 [00:01<00:00, 854.67 examples/s]\n",
      "Map: 100%|██████████| 8155/8155 [00:11<00:00, 738.92 examples/s]\n",
      "Map: 100%|██████████| 1020/1020 [00:01<00:00, 751.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "########################################################## DATA WORK\n",
    "print(\"_________________________________\")\n",
    "print(\"Preapring the Tokenizer\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id + \"/Tokenizer\")\n",
    "if tokenizer.pad_token is None and \"Llama\" in model_id: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "elif tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = open(model_id + \"/Tokenizer/chat_template.jinja\").read()\n",
    "\n",
    "# print(tokenizer.chat_template)\n",
    "# print(tokenizer.apply_chat_template(\"Hello World\", tokenize=False, add_generation_prompt=True, return_tensors=\"pt\"))\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=512):\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # for the cross entropy loss\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "print(\"----------Preparing the Data-----------------\")\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading and filtering the Data\")\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "times_array = list(df[\"time\"].unique())\n",
    "datasets = []\n",
    "dataset_names = list(df[\"task\"].unique())\n",
    "\n",
    "for task in training_order:\n",
    "\n",
    "    time_ds = []\n",
    "    for split in df[\"split\"].unique():\n",
    "\n",
    "        split_df = df[(df[\"split\"] == split) & (df[\"task\"] == task)]\n",
    "        hf_split = Dataset.from_pandas(split_df)\n",
    "        time_ds.append(hf_split)\n",
    "    datasets.append(time_ds)\n",
    "\n",
    "datasets_test = []\n",
    "for i, task in enumerate(testing_order):\n",
    "\n",
    "    split_df = df[(df[\"split\"] == \"test\") & (df[\"task\"] == task)]\n",
    "    hf_split = Dataset.from_pandas(split_df)\n",
    "    datasets_test.append({testing_order[i]: hf_split})\n",
    "\n",
    "hf_datasets = []\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(\"dataset\")\n",
    "\n",
    "    hf_ds = DatasetDict({dataset[0][\"split\"][0]: dataset[0], \n",
    "                        dataset[1][\"split\"][0]: dataset[1],\n",
    "                        dataset[2][\"split\"][0]: dataset[2]})\n",
    "    hf_ds_name = training_order[i]\n",
    "    hf_datasets.append({hf_ds_name: hf_ds})\n",
    "\n",
    "hf_datasets_processed = []\n",
    "for ds in hf_datasets:\n",
    "    ds_dict = {}\n",
    "    for task_name, hf_data in ds.items():\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            ds_dict[task_name][split] = hf_data[split].map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)\n",
    "    hf_datasets_processed.append(ds_dict)\n",
    "\n",
    "n_samples_per_ds = [\n",
    "    len(hf_time[\"train\"])\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items() \n",
    "]\n",
    "\n",
    "for ds in hf_datasets_processed:\n",
    "    for task_name, hf_data in ds.items():\n",
    "        for split in hf_data:\n",
    "            hf_data[split].set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \n",
    "                \"target\", \"implied_statement\", \"split\", \"time\", \"task\",\n",
    "                \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "hf_datasets_no_cols = []\n",
    "for ds in hf_datasets_processed:\n",
    "    ds_dict = {}\n",
    "    for task_name, hf_data in ds.items():\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            if split != \"test\":\n",
    "                ds_dict[task_name][split] = hf_data[split].remove_columns(cols_to_remove)\n",
    "    hf_datasets_no_cols.append(ds_dict)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e047325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADERS AT THE END OF PROCESSING\n",
      "[{'explicit_hs': {'train': <torch.utils.data.dataloader.DataLoader object at 0x000001A9CA23C770>,\n",
      "                  'validation': <torch.utils.data.dataloader.DataLoader object at 0x000001A9BF9865A0>},\n",
      "  'test': Dataset({\n",
      "    features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
      "    num_rows: 720\n",
      "})},\n",
      " {'implicit_hs': {'train': <torch.utils.data.dataloader.DataLoader object at 0x000001A9CA23C3B0>,\n",
      "                  'validation': <torch.utils.data.dataloader.DataLoader object at 0x000001A9CA23CD40>},\n",
      "  'test': Dataset({\n",
      "    features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
      "    num_rows: 1020\n",
      "})}]\n",
      "\n",
      "TEST DATA AT THE END OF PROCESSING\n",
      "[{'explicit_hs': Dataset({\n",
      "    features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
      "    num_rows: 720\n",
      "})},\n",
      " {'implicit_hs': Dataset({\n",
      "    features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
      "    num_rows: 1020\n",
      "})}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_loaders = []\n",
    "for ds in hf_datasets_no_cols:\n",
    "    ds_dict = {}\n",
    "    for task_name, hf_data in ds.items():\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            if split != \"test\":\n",
    "                data_loader = DataLoader(ds[task_name][split], collate_fn=data_collator, batch_size=batch_size)\n",
    "                ds_dict[task_name][split] = data_loader\n",
    "    data_loaders.append(ds_dict)\n",
    "\n",
    "for i, ds in enumerate(hf_datasets):\n",
    "    for task_name, hf_data in ds.items():\n",
    "        for split in hf_data:\n",
    "            if split == \"test\":\n",
    "                data_loaders[i][\"test\"] = hf_data[split]\n",
    "\n",
    "print(\"DATA LOADERS AT THE END OF PROCESSING\")\n",
    "pp(data_loaders)\n",
    "print()\n",
    "print(\"TEST DATA AT THE END OF PROCESSING\")\n",
    "pp(datasets_test)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b630b",
   "metadata": {},
   "source": [
    "## Preparing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db84575",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_array = []\n",
    "\n",
    "for i in range(len(training_order)):\n",
    "    epochs_array.append(n_epochs)\n",
    "\n",
    "ks_array = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab1b6c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Loading the model and model config with LoRA and 4-bit quantization nf4\n",
      "trainable params: 1,638,400 || all params: 363,459,520 || trainable%: 0.4508\n",
      "_________________________________\n",
      "CONTINUAL LEARNING EXPERIMENT SET UP\n",
      "model :\t Models/SmolLM2-360M-Instruct\n",
      "\n",
      "training_order :\t ['explicit_hs', 'implicit_hs']\n",
      "\n",
      "testing_order :\t ['explicit_hs', 'implicit_hs']\n",
      "\n",
      "zero_testing :\t []\n",
      "\n",
      "epochs :\t [8, 8]\n",
      "\n",
      "cl_technique :\t ewc\n",
      "\n",
      "type_experiment :\t from_expl_to_impl\n",
      "\n",
      "batch_size :\t 16\n",
      "\n",
      "learning_rate :\t 0.0001\n",
      "\n",
      "results :\t []\n",
      "\n",
      "_________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Loading the model and model config with LoRA and 4-bit quantization nf4\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n",
    "\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "if cl_technique in [\"ewc\", \"agem\", \"lwf\", \"mas\"]:\n",
    "    cl_hyperparams = {\n",
    "    \"ewc\": {\"ewc_lambda\":1500},\n",
    "    \"agem\": {\"mem_size\":100},\n",
    "    \"lwf\": {\"lwf_lambda\":1,\n",
    "            \"temperature\":2},\n",
    "    \"mas\": {\"mas_lambda\":1000}\n",
    "    }\n",
    "\n",
    "    cl_params = cl_hyperparams[cl_technique]\n",
    "    hyper_param_str = \"=\".join([str(k) + \"-\" + str(v) for k, v in cl_params.items()])\n",
    "\n",
    "    model = AutoContinualLearner(model_id + \"/Model\", device, bnb_config)\n",
    "    model.init_cl(technique=cl_technique, lora_config=config, **cl_params)\n",
    "\n",
    "else:\n",
    "    cl_params = {\"NA\": \"NA\"}\n",
    "    hyper_param_str = \"NA\"\n",
    "\n",
    "    model = AutoContinualLearner(model_id + \"/Model\", device, bnb_config)\n",
    "    model.init_cl(technique=cl_technique, lora_config=config)\n",
    "\n",
    "n_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "\n",
    "hf_datasets = data_loaders\n",
    "\n",
    "zero_testing_order = [dataset for dataset in testing_order if dataset not in training_order]\n",
    "\n",
    "results = {\n",
    "            \"model\": model_id,\n",
    "            \"training_order\": training_order,\n",
    "            \"testing_order\": testing_order,\n",
    "            \"zero_testing\":zero_testing_order,\n",
    "            \"epochs\": epochs_array,\n",
    "            \"exp_setup\": exp_setup,\n",
    "            \"cl_technique\": cl_technique,\n",
    "            \"type_experiment\": type_experiment,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": lr,\n",
    "            \"results\": []\n",
    "            }\n",
    "\n",
    "print(\"CONTINUAL LEARNING EXPERIMENT SET UP\")\n",
    "for k, v in results.items():\n",
    "    if k != \"exp_setup\":\n",
    "        print(k, \":\\t\", v)\n",
    "        print()\n",
    "\n",
    "data_loaders_train = []\n",
    "data_loaders_val = []\n",
    "test_datasets = []\n",
    "for time, ds in enumerate(training_order):\n",
    "    # print(\"training order\")\n",
    "    # print(training_order)\n",
    "    # print(\"ds\")\n",
    "    # print(ds)\n",
    "    # print(\"hf_datasets\")\n",
    "    # print(hf_datasets[time].keys())\n",
    "    data_loaders_train.append(hf_datasets[time][ds][\"train\"])\n",
    "    data_loaders_val.append(hf_datasets[time][ds][\"validation\"])\n",
    "    test_datasets.append({\"test\":hf_datasets[time][\"test\"]})\n",
    "# print(\"TEST DATASETS BEFORE STARTING THE EXPERIENCE\")\n",
    "# print(test_datasets)\n",
    "\n",
    "test_results = []\n",
    "train_results = []\n",
    "\n",
    "n_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "\n",
    "hf_datasets = data_loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514579e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Starting Experience----------------\n",
      "------------------TIME 0------------------------\n",
      "\n",
      "Epochs in the current time: 8\n",
      "Number of training samples: 5752\n",
      "Current Dataset: explicit_hs\n",
      "\n",
      "_________________________________\n",
      "Training the model\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 10.45 GiB is allocated by PyTorch, and 188.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Switching Batch Size to Unsqueezed\n"
     ]
    }
   ],
   "source": [
    "for time, current_training_dataset in enumerate(data_loaders_train): # current tr_ds is a string!!\n",
    "    print(\"------------------Starting Experience----------------\")\n",
    "    print(f\"------------------TIME {time}------------------------\")\n",
    "    print()\n",
    "    # torch.cuda.ipc_collect()\n",
    "    # torch.cuda.empty_cache()\n",
    "    n_epochs = epochs_array[time]\n",
    "    if ks_array != None:\n",
    "        n_samples = ks_array[time]\n",
    "    else:\n",
    "        n_samples = n_samples_per_ds[time]\n",
    "\n",
    "    current_dataset_name = training_order[time]\n",
    "    current_testing_dataset = testing_order[time]\n",
    "\n",
    "    print(f\"Epochs in the current time: {n_epochs}\\nNumber of training samples: {n_samples}\\nCurrent Dataset: {current_dataset_name}\")\n",
    "    print()\n",
    "\n",
    "    train_loader=data_loaders_train[time]\n",
    "    validation_loader=data_loaders_val[time]\n",
    "\n",
    "\n",
    "    print(\"_________________________________\")\n",
    "    print(\"Training the model\")\n",
    "    print()\n",
    "\n",
    "    global_training_losses = []\n",
    "    global_validation_losses = []\n",
    "    current_testing_dataset=testing_order[time]\n",
    "\n",
    "    model.train()\n",
    "    with torch.amp.autocast('cuda', dtype=torch.float32):\n",
    "    # for task in tasks/dataset - train, eval\n",
    "        for epoch in tqdm(range(n_epochs)):\n",
    "            if epoch > 0:\n",
    "                continue\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "            epoch_validation_losses = []\n",
    "            train_losses = []\n",
    "\n",
    "            print(\"Epoch: \", epoch)\n",
    "\n",
    "            for i, batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                batch_unsqueezed = batch\n",
    "                # print(\"\\tBatch: \", i)\n",
    "                batch = {k:torch.squeeze(v).to(device) for k,v in batch.items()}\n",
    "\n",
    "                # print(\"Squeezed Batch\")\n",
    "                for k, v in batch.items():\n",
    "                    if v.shape[0] != batch_size:\n",
    "                        print(f\"{k}: {v.shape}\")\n",
    "                        print()\n",
    "                        continue\n",
    "                    \n",
    "\n",
    "\n",
    "                # print(\"Unsqueezed Batch\")\n",
    "                # for k, v in batch_unsqueezed.items():\n",
    "                #     print(f\"{k}: {v.shape}\")\n",
    "\n",
    "                # print(batch[\"input_ids\"].shape)\n",
    "                # print(batch[\"attention_mask\"].shape)\n",
    "                # print(batch[\"labels\"].shape)\n",
    "                try:\n",
    "                    output = model.model(**batch)\n",
    "                    # print(output)\n",
    "                    logits = output.logits\n",
    "                    # print(\"Shape Logits\")\n",
    "                    # print(logits.shape)\n",
    "                    # print(\"Shape Labels\")\n",
    "                    # print(batch[\"labels\"].shape)\n",
    "                    loss = loss_f(logits, batch[\"labels\"])\n",
    "\n",
    "                    if i == 0:\n",
    "                        print(\"Checking that the model type is the continual learner to do the cls\")\n",
    "                        print(type(model))\n",
    "                        print(type(model.cl))\n",
    "                        print(model.cl)\n",
    "                        # print(dir(model.module))\n",
    "                    if model.cl:\n",
    "                        batch['logits'] = logits  # needed for LwF\n",
    "                        loss += modelodule.cl.compute_regularization(batch)\n",
    "                        model.cl.pre_backward(batch)\n",
    "                    # print(\"CL regularization and backward computed\")\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "\n",
    "                    # needed agem (A-GEM)\n",
    "                    if model.module.cl:\n",
    "                        model.module.cl.post_backward()\n",
    "                    # print(\"CL post backward computed\")\n",
    "\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    train_losses.append(loss.detach().item())\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print()\n",
    "                    print(e)\n",
    "                    print(\"Switching Batch Size to Unsqueezed\")\n",
    "\n",
    "                #     output = model.module.model(**batch_unsqueezed)\n",
    "                #     # print(output)\n",
    "                #     logits = output.logits\n",
    "                #     # print(\"Shape Logits\")\n",
    "                #     # print(logits.shape)\n",
    "                #     # print(\"Shape Labels\")\n",
    "                #     # print(batch_unsqueezed[\"labels\"].shape)\n",
    "                #     loss = loss_f(logits, batch_unsqueezed[\"labels\"])\n",
    "                #     # print(dir(model.module))\n",
    "                #     if model.module.cl:\n",
    "                #         batch_unsqueezed['logits'] = logits  # needed for LwF\n",
    "                #         loss += model.module.cl.compute_regularization(batch_unsqueezed)\n",
    "                #         model.module.cl.pre_backward(batch_unsqueezed)\n",
    "                #     print(\"CL regularization and backward computed\")\n",
    "\n",
    "                #     loss.backward()\n",
    "\n",
    "\n",
    "                #     # needed agem (A-GEM)\n",
    "                #     if model.module.cl:\n",
    "                #         model.module.cl.post_backward()\n",
    "                #     print(\"CL post backward computed\")\n",
    "\n",
    "                #     optimizer.step()\n",
    "                #     optimizer.zero_grad()\n",
    "\n",
    "                #     train_losses.append(loss.detach().item())\n",
    "\n",
    "            # epoch_loss = sum(train_losses) / len(train_losses) # loss on current device\n",
    "\n",
    "            # epoch_loss_tensor = torch.tensor(epoch_loss, device=device)\n",
    "\n",
    "            # print(f\"Epoch Loss: {epoch_loss_tensor.item()}\")\n",
    "            # global_training_losses.append(epoch_loss_tensor.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32):\n",
    "            with torch.no_grad():\n",
    "\n",
    "                print(\"_________________________________\")\n",
    "                print(\"Validating the model\")\n",
    "                print()\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                val_losses = [] # val loss for each batch\n",
    "\n",
    "                for i, batch in enumerate(tqdm(validation_loader)):\n",
    "                    \n",
    "                    # batch.to(device)\n",
    "                    batch_unsqueezed = batch\n",
    "                    # print(\"\\tBatch: \", i)\n",
    "                    batch = {k:torch.squeeze(v).to(device) for k,v in batch.items()}\n",
    "\n",
    "                    # print(\"Squeezed Batch\")\n",
    "                    for k, v in batch.items():\n",
    "                        if v.shape[0] != batch_size:\n",
    "                            print(f\"{k}: {v.shape}\")\n",
    "                            print()\n",
    "                            continue\n",
    "\n",
    "                    # print(\"Unsqueezed Batch\")\n",
    "                    # for k, v in batch_unsqueezed.items():\n",
    "                    #     print(f\"{k}: {v.shape}\")\n",
    "\n",
    "                    try:\n",
    "                        output = model.model(**batch)\n",
    "                        # print(output)\n",
    "                        logits = output.logits\n",
    "                        # print(\"Shape Logits\")\n",
    "                        # print(logits.shape)\n",
    "                        # print(\"Shape Labels\")\n",
    "                        # print(batch[\"labels\"].shape)\n",
    "                        val_loss = loss_f(logits, batch[\"labels\"])\n",
    "                        val_losses.append(val_loss.detach().item())\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(\"Switching Batch Size to unsqueezed\")\n",
    "                #         output = model.model(**batch_unsqueezed)\n",
    "                #         logits = output.logits\n",
    "                #         val_loss = loss_f(logits, batch_unsqueezed[\"labels\"])\n",
    "\n",
    "                #         val_losses.append(val_loss.detach().item())\n",
    "\n",
    "                        print()\n",
    "                        print(e)\n",
    "                \n",
    "                # val_loss_epoch = sum(val_losses)/len(val_losses)\n",
    "                # val_loss_tensor = torch.tensor(val_loss_epoch, device=device)\n",
    "\n",
    "                # print(\"_________________________________\")\n",
    "                # print(\"Validation completed\")\n",
    "                # print()\n",
    "                # print(f\"Validation Loss: {val_loss_tensor.item()}\")\n",
    "                # print(\"_________________________________\")\n",
    "\n",
    "                # val_loss = val_loss_tensor.item()\n",
    "                # epoch_validation_losses.append(val_loss)\n",
    "                # global_validation_losses.append(val_loss)\n",
    "\n",
    "        # print()\n",
    "        # print(\"---------------------TRAINING ENDED---------------\")\n",
    "        # print(\"Final Training Losses:\", global_training_losses)\n",
    "        # print(\"Final Validation Losses:\", global_validation_losses)\n",
    "\n",
    "        # if model.module.cl:\n",
    "        #     model.module.cl.post_task_update(train_loader)\n",
    "\n",
    "        # print(\"-----------POST TRAINING CL UPDATES COMPLETED---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b3791",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40879666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hf():\n",
    "    \n",
    "    load_dotenv(\"env_vars.env\")\n",
    "    hf_token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    HfFolder.save_token(hf_token)\n",
    "    return print(whoami()[\"name\"])\n",
    "\n",
    "def setup():\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return local_rank\n",
    "\n",
    "def save_results_csv(df, experiment_name, model_id, cl_technique, result_type=\"specific\"):\n",
    "\n",
    "    cl_technique_clean = cl_technique.replace(\" + \", \"__\")\n",
    "    id_ = model_id.replace(\"/\", \"-\") + \"_\" + cl_technique_clean + \"_\" + str(date.today())\n",
    "    id_clean = experiment_name + id_.replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\",\"-\") + result_type + \".csv\"\n",
    "    df.to_csv(id_clean, index=False)\n",
    "    return print(\"Saved in path: \", id_clean)\n",
    "\n",
    "def clean_cl_name(cl_name):\n",
    "\n",
    "    regex = r'<(?:[\\w\\.]+)?\\.([\\w]+) object at'\n",
    "    matches =   re.findall(regex, cl_name)\n",
    "    clean_string = \" + \".join(matches)\n",
    "    return clean_string\n",
    "\n",
    "def clean_metric_name(metric_name):\n",
    "\n",
    "    reg = r\"\\s([a-z_1]+)\\s\"\n",
    "    match_ = re.search(reg, metric_name)\n",
    "    clean_str = match_.group().strip()\n",
    "\n",
    "    return clean_str\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n",
    "\n",
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def get_probability_distribution(logits):\n",
    "    probability_dist = Softmax(dim=-1)(logits)\n",
    "    return probability_dist\n",
    "\n",
    "loss_fn = CrossEntropyLoss(ignore_index=-100) # ignore the left pad tokens\n",
    "def loss_f(logits, labels):\n",
    "\n",
    "    flat_logits = logits.view(-1, logits.size(-1))\n",
    "    flat_labels = labels.view(-1)\n",
    "\n",
    "    loss = loss_fn(flat_logits, flat_labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def translate_prediction_to_label(text):\n",
    "    if \"NOT HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    elif \"NOT_HATEFUL\" in text:\n",
    "        text_clean = text.replace(\"NOT_HATEFUL\", \"\")\n",
    "        if \"HATEFUL\" in text_clean or \"HATEFUAL\" in text_clean:\n",
    "            return 2\n",
    "        else: \n",
    "            return 0\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76173a",
   "metadata": {},
   "source": [
    "## Data Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a33267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss, Softmax\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b513bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Models/SmolLM2-360M-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                    load_in_4bit= True,\n",
    "                                    bnb_4bit_quant_type= \"nf4\",\n",
    "                                    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                    bnb_4bit_use_double_quant= True,\n",
    "                                )\n",
    "                                \n",
    "lora_r = 8\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLTechniques:\n",
    "    \"\"\"Container for all continual learning techniques\"\"\"\n",
    "\n",
    "    def __init__(self, model, device, technique=\"none\",\n",
    "                ewc_lambda=1000,\n",
    "                mem_size=100,\n",
    "                lwf_lambda=1,\n",
    "                temperature=2,\n",
    "                mas_lambda=1000):\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.technique = technique.lower()\n",
    "\n",
    "        # Initialize selected technique\n",
    "        if self.technique == \"ewc\":\n",
    "            self._init_ewc(ewc_lambda)\n",
    "        elif self.technique == \"agem\":\n",
    "            self._init_agem(mem_size)\n",
    "        elif self.technique == \"lwf\":\n",
    "            self._init_lwf(lwf_lambda, temperature)\n",
    "        elif self.technique == \"mas\":\n",
    "            self._init_mas(mas_lambda)\n",
    "\n",
    "    def _init_ewc(self, ewc_lambda):\n",
    "        \"\"\"Elastic Weight Consolidation\"\"\"\n",
    "        self.ewc_lambda = ewc_lambda\n",
    "        self.params = {n: p.clone().detach()\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if p.requires_grad}\n",
    "        self.fisher = {n: torch.zeros_like(p)\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if p.requires_grad}\n",
    "\n",
    "    def _init_agem(self, mem_size):\n",
    "        \"\"\"Average Gradient Episodic Memory\"\"\"\n",
    "        self.mem_size = mem_size\n",
    "        self.memory = []\n",
    "\n",
    "    def _init_lwf(self, lwf_lambda, temperature):\n",
    "        \"\"\"Learning Without Forgetting\"\"\"\n",
    "        self.lwf_lambda = lwf_lambda\n",
    "        self.temperature = temperature\n",
    "        self.old_model = None\n",
    "\n",
    "    def _init_mas(self, mas_lambda):\n",
    "        \"\"\"Memory Aware Synapses\"\"\"\n",
    "        self.mas_lambda = mas_lambda\n",
    "        self.importance = {n: torch.zeros_like(p)\n",
    "                        for n, p in self.model.named_parameters()\n",
    "                        if p.requires_grad}\n",
    "        self.old_params = deepcopy(self.importance)\n",
    "\n",
    "    def compute_regularization(self, inputs=None):\n",
    "        \"\"\"Compute CL regularization term\"\"\"\n",
    "        if self.technique == \"ewc\":\n",
    "            penalty = 0\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    penalty += (self.fisher[n] * (p - self.params[n]).pow(2)).sum()\n",
    "            return self.ewc_lambda * penalty\n",
    "\n",
    "        elif self.technique == \"lwf\" and self.old_model:\n",
    "            with torch.no_grad():\n",
    "                logits = inputs['logits']\n",
    "                actual_inputs = {k:torch.squeeze(v).to(self.device) for k,v in inputs.items() if k != \"logits\"}\n",
    "                old_outputs = self.old_model(**actual_inputs)\n",
    "            return self.lwf_lambda * KLDivLoss(reduction='batchmean')(\n",
    "                torch.log_softmax(logits/self.temperature, dim=1),\n",
    "                torch.softmax(old_outputs.logits/self.temperature, dim=1)\n",
    "            ) * (self.temperature ** 2)\n",
    "\n",
    "        elif self.technique == \"mas\":\n",
    "            penalty = 0\n",
    "            for n, p in self.model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    penalty += (self.importance[n] * (p - self.old_params[n]).pow(2)).sum()\n",
    "            return self.mas_lambda * penalty\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def pre_backward(self, inputs=None):\n",
    "        \"\"\"Operations before backward pass\"\"\"\n",
    "        if self.technique == \"agem\" and self.memory:\n",
    "            # Store current gradient\n",
    "            self.model.zero_grad()\n",
    "            for inputs_mem, labels_mem in self.memory:\n",
    "                inputs_mem = {k:torch.squeeze(v).to(self.device) for k,v in inputs_mem.items()}\n",
    "                labels_mem = torch.squeeze(labels_mem).to(self.device)\n",
    "                outputs = self.model(**inputs_mem)\n",
    "                loss = loss_f(outputs.logits, labels_mem)\n",
    "                loss.backward()\n",
    "\n",
    "            self.ref_grad = [p.grad.clone() for p in self.model.parameters() if p.requires_grad] # i think this should be with req grad\n",
    "            self.model.zero_grad()\n",
    "\n",
    "    def post_backward(self):\n",
    "        \"\"\"Operations after backward pass\"\"\"\n",
    "        if self.technique == \"agem\" and hasattr(self, 'ref_grad'):\n",
    "            # Project gradients\n",
    "            dot_product = sum(torch.sum(p.grad * g_ref)\n",
    "                        for p, g_ref in zip(self.model.parameters(), self.ref_grad) if p.requires_grad)\n",
    "            ref_norm = sum(torch.sum(g_ref * g_ref) for g_ref in self.ref_grad)\n",
    "\n",
    "            if dot_product < 0:  # Negative interference\n",
    "                scale = dot_product / (ref_norm + 1e-8)\n",
    "                for p, g_ref in zip(self.model.parameters(), self.ref_grad):\n",
    "                    if p.grad is not None and p.requires_grad:\n",
    "                        p.grad -= scale * g_ref\n",
    "\n",
    "    def post_task_update(self, dataloader=None):\n",
    "        \"\"\"Update after each task\"\"\"\n",
    "        if self.technique == \"ewc\":\n",
    "            # Compute Fisher information\n",
    "            self.model.eval()\n",
    "            for batch in dataloader:\n",
    "                self.model.zero_grad()\n",
    "                inputs = {k:torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                # outputs = self.model(**inputs, labels=labels)\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "                loss = loss_f(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        self.fisher[n] += p.grad.pow(2) / len(dataloader)\n",
    "\n",
    "            # Update stored parameters\n",
    "            self.params = {n: p.clone().detach()\n",
    "                        for n, p in self.model.named_parameters()\n",
    "                        if p.requires_grad}\n",
    "\n",
    "        elif self.technique == \"agem\":\n",
    "            # Update memory buffer\n",
    "            self.memory = []\n",
    "            for batch in dataloader:\n",
    "                inputs = {k: torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "                labels = torch.squeeze(batch['labels']).to(self.device)\n",
    "                self.memory.append((inputs, labels))\n",
    "                if len(self.memory) >= self.mem_size:\n",
    "                    break\n",
    "\n",
    "        elif self.technique == \"lwf\":\n",
    "            # Save model snapshot\n",
    "            self.old_model = deepcopy(self.model)\n",
    "            self.old_model.eval()\n",
    "\n",
    "        elif self.technique == \"mas\":\n",
    "            # Update importance weights\n",
    "            self.model.eval()\n",
    "            for batch in dataloader:\n",
    "                self.model.zero_grad()\n",
    "                inputs = {k: torch.squeeze(v).to(self.device) for k, v in batch.items()\n",
    "                        if k in ['input_ids', 'attention_mask']}\n",
    "\n",
    "                outputs = self.model(**inputs)\n",
    "                # does this need a loss?????????????\n",
    "                torch.norm(outputs.logits, p=2, dim=1).mean().backward()\n",
    "\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        self.importance[n] += p.grad.abs() / len(dataloader)\n",
    "\n",
    "            # Update stored parameters\n",
    "            self.old_params = {n: p.clone().detach()\n",
    "                            for n, p in self.model.named_parameters()\n",
    "                            if p.requires_grad}\n",
    "\n",
    "class AutoContinualLearner(nn.Module):\n",
    "    def __init__(self, model_name, device, quantization_config, torch_dtype=torch.bfloat16):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            torch_dtype=torch_dtype\n",
    "        )\n",
    "        self.n_initial_params = sum(t.numel() for t in self.model.parameters())\n",
    "        self.n_trainable_params_initial = sum(t.numel() for t in self.model.parameters() if t.requires_grad)\n",
    "        self.cl = None\n",
    "\n",
    "    def init_cl(self, technique, lora_config, **kwargs):\n",
    "        \"\"\"Init the continual learning technique\"\"\"\n",
    "        self.model = get_peft_model(self.model, lora_config).to(self.device)\n",
    "        self.n_params_lora = sum(t.numel() for t in self.model.parameters())\n",
    "        self.n_trainable_params_lora = sum(t.numel() for t in self.model.parameters() if t.requires_grad)\n",
    "        self.model.print_trainable_parameters()\n",
    "        self.cl = CLTechniques(self.model, self.device, technique, **kwargs)\n",
    "    def forward(self, **kwargs):\n",
    "        return self.model(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041aca0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_technique = \"ewc\"\n",
    "\n",
    "if cl_technique in [\"ewc\", \"agem\", \"lwf\", \"mas\"]:\n",
    "    cl_hyperparams = {\n",
    "    \"ewc\": {\"ewc_lambda\":1500},\n",
    "    \"agem\": {\"mem_size\":100},\n",
    "    \"lwf\": {\"lwf_lambda\":1,\n",
    "            \"temperature\":2},\n",
    "    \"mas\": {\"mas_lambda\":1000}\n",
    "    }\n",
    "\n",
    "    cl_params = cl_hyperparams[cl_technique]\n",
    "    hyper_param_str = \"=\".join([str(k) + \"-\" + str(v) for k, v in cl_params.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27232141",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca1fd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,638,400 || all params: 363,459,520 || trainable%: 0.4508\n"
     ]
    }
   ],
   "source": [
    "model = AutoContinualLearner(model_id + \"/Model\", device, bnb_config)\n",
    "model.init_cl(technique=cl_technique, lora_config=config, **cl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id + \"/Tokenizer\")\n",
    "if tokenizer.pad_token is None and \"Llama\" in model_id: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "elif tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = open(model_id + \"/Tokenizer/chat_template.jinja\").read()\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=512):\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # for the cross entropy loss\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f15e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"df_from_exp_to_imp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848352f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Preparing the Data-----------------\n",
      "_________________________________\n",
      "Loading and filtering the Data\n"
     ]
    }
   ],
   "source": [
    "print(\"----------Preparing the Data-----------------\")\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading and filtering the Data\")\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "testing_order = [\"explicit_hs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d6bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task\n",
      "explicit_hs\n"
     ]
    }
   ],
   "source": [
    "datasets_test = []\n",
    "for i, task in enumerate(testing_order):\n",
    "    print(\"Task\")\n",
    "    print(task)\n",
    "    split_df = df[(df[\"split\"] == \"test\") & (df[\"task\"] == task)]\n",
    "    hf_split = Dataset.from_pandas(split_df)\n",
    "    datasets_test.append({testing_order[i]: hf_split})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91ce08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'explicit_hs': Dataset({\n",
       "      features: ['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'task', 'formatted_prompt', 'label', '__index_level_0__'],\n",
       "      num_rows: 720\n",
       "  })}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a778f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = datasets_test[0][\"explicit_hs\"][21][\"formatted_prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are a social media content moderator.\\n'\n",
      " 'INSTRUCTION: The following is a social media message that needs to be '\n",
      " 'classified with the label HATEFUL or NOT HATEFUL.\\n'\n",
      " \"MESSAGE: does this mean white south africans aren't actually white  because \"\n",
      " \"they're born in pri\\n\"\n",
      " 'OUTPUT AND FORMAT: your output should be just the label.')\n"
     ]
    }
   ],
   "source": [
    "pp(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": example}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b5abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|im_start|>system\\n'\n",
      " 'You are a helpful assistant<|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " 'You are a social media content moderator.\\n'\n",
      " 'INSTRUCTION: The following is a social media message that needs to be '\n",
      " 'classified with the label HATEFUL or NOT HATEFUL.\\n'\n",
      " \"MESSAGE: does this mean white south africans aren't actually white  because \"\n",
      " \"they're born in pri\\n\"\n",
      " 'OUTPUT AND FORMAT: your output should be just the label.<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n')\n"
     ]
    }
   ],
   "source": [
    "chat_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "pp(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750fa788",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
    "input_ids_tokenized = input_dict[\"input_ids\"]\n",
    "attention_mask = input_dict[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c259e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = model.model.generate(input_ids=input_ids_tokenized, \n",
    "                                        attention_mask=attention_mask, \n",
    "                                        top_p=0.9, \n",
    "                                        temperature=0.6, \n",
    "                                        max_new_tokens=10,\n",
    "                                        return_dict_in_generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cf151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,  9690,   198,  2683,   359,   253,  5356, 11173,     2,   198,\n",
       "            1,  4093,   198,  2683,   359,   253,  1329,  2902,  2627, 45057,\n",
       "           30,   198,  2113, 47687,  4171,    42,   378,  1695,   314,   253,\n",
       "         1329,  2902,  3714,   338,  1923,   288,   325,  9827,   351,   260,\n",
       "         4368,   407,  9081,    54,  6565,   355,  9695,   407,  9081,    54,\n",
       "         6565,    30,   198, 26826, 41601,    42,  1072,   451,  1441,  2537,\n",
       "         4203, 46230,   487,  4775,   982,  2390,  2537,   216,   975,   502,\n",
       "         2316,  3988,   281, 24993,   198, 44724,  7485, 12111, 40560,    42,\n",
       "          469,  3124,   868,   325,   915,   260,  4368,    30,     2,   198,\n",
       "            1,   520,  9531,   198, 18083,   407,  9081,    54,  6565,     2],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = generated_tokens[0]\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tokenizer.decode(seq[input_ids_tokenized.shape[1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2797d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NOT HATEFUL'\n"
     ]
    }
   ],
   "source": [
    "pp(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80944123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Testing the model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# prompt_plus_messages = base_prompt.format(clean_post)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# print(\"FORMATTED PROMPT\")\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# print(formatted_prompt)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     38\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     39\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: formatted_prompt}\n\u001b[0;32m     40\u001b[0m ]\n\u001b[1;32m---> 41\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# print(\"CHAT TEMPLATE COMPUTED\")\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# print(chat_template)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m tokenizer(chat_template, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1652\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1649\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinue_final_message is not compatible with return_assistant_tokens_mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1651\u001b[0m template_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}  \u001b[38;5;66;03m# kwargs overwrite special tokens if both are present\u001b[39;00m\n\u001b[1;32m-> 1652\u001b[0m rendered_chat, generation_indices \u001b[38;5;241m=\u001b[39m \u001b[43mrender_jinja_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinue_final_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinue_final_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1659\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1660\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[0;32m   1664\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m rendered_chat[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\chat_template_utils.py:498\u001b[0m, in \u001b[0;36mrender_jinja_template\u001b[1;34m(conversations, tools, documents, chat_template, return_assistant_tokens_mask, continue_final_message, add_generation_prompt, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 498\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[0;32m    506\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jinja2\\environment.py:1295\u001b[0m, in \u001b[0;36mTemplate.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\jinja2\\environment.py:942\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[1;32m--> 942\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[1;32m<template>:23\u001b[0m, in \u001b[0;36mtop-level template code\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "\n",
    "predictions_test = []\n",
    "labels_test = []\n",
    "predicted_strings = []\n",
    "labels_strings = []\n",
    "full_generation = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # print(\"TESTING DS\")\n",
    "    # print(ds)\n",
    "    # print()\n",
    "    # for i, test_item in enumerate(ds[\"test\"]):\n",
    "    for i, test_item in enumerate(datasets_test[0].values()):\n",
    "        # print(\"TESTING ITEM\")\n",
    "        # print(test_item)\n",
    "        # print(i)\n",
    "        target_label = test_item[\"label\"]\n",
    "        labels_strings.append(target_label)\n",
    "        # print(\"TARGET LABEL\")\n",
    "        # print(target_label)\n",
    "        if target_label == \"NOT HATEFUL\":\n",
    "            target_label = 0\n",
    "        elif target_label == \"HATEFUL\":\n",
    "            target_label = 1\n",
    "        \n",
    "        labels_test.append(target_label)\n",
    "\n",
    "        formatted_prompt = test_item[\"formatted_prompt\"]\n",
    "        # prompt_plus_messages = base_prompt.format(clean_post)\n",
    "        # print(\"FORMATTED PROMPT\")\n",
    "        # print(formatted_prompt)\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "        chat_template = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        # print(\"CHAT TEMPLATE COMPUTED\")\n",
    "        # print(chat_template)\n",
    "        input_dict = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
    "        input_ids_tokenized = input_dict[\"input_ids\"]\n",
    "        attention_mask = input_dict[\"attention_mask\"]\n",
    "        \n",
    "        # print(\"TOKENIZED CHAT TEMPLATE COMPUTED\")\n",
    "        # print(input_ids_tokenized)\n",
    "        # print(type(input_ids_tokenized))\n",
    "        # if input_ids_tokenized.shape[0] == 1:\n",
    "        #     print(\"wrong size\")\n",
    "        #     input_ids_tokenized = input_ids_tokenized.squeeze(0)\n",
    "        #     attention_mask = attention_mask.squeeze(0)\n",
    "        # print(\"NEW SHAPE\")\n",
    "        # print(input_ids_tokenized.shape)\n",
    "        # print(attention_mask.shape)\n",
    "        # ######################\n",
    "        # print(\"----------------right beforeoutput---------------------------------------\")\n",
    "        # # print(model)\n",
    "        # # print(model.module)\n",
    "        # # print(dir(model))\n",
    "        # # print(dir(model.module))\n",
    "        # # print(help(model.module.generate))\n",
    "        # print(model.module.generate(input_ids=input_ids_tokenized, \n",
    "        #                                 attention_mask=attention_mask, \n",
    "        #                                 top_p=0.9, \n",
    "        #                                 temperature=0.6, \n",
    "        #                                 max_new_tokens=10,\n",
    "        #                                 return_dict_in_generate=False))\n",
    "        # print(\"----------------right after output---------------------------------------\")\n",
    "        output = model.module.model.generate(input_ids=input_ids_tokenized, \n",
    "                                        attention_mask=attention_mask, \n",
    "                                        top_p=0.9, \n",
    "                                        temperature=0.6, \n",
    "                                        max_new_tokens=10,\n",
    "                                        return_dict_in_generate=False)\n",
    "                \n",
    "        # pred = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        # print(\"OUTPUT COMPUTED\")\n",
    "        # print(output)\n",
    "        # print(type(output))\n",
    "        seq = output[0]\n",
    "        # print(tokenizer.decode(seq, skip_special_tokens=True).strip())\n",
    "        pred = tokenizer.decode(seq[input_ids_tokenized.shape[1]:], skip_special_tokens=True)\n",
    "        print(pred)\n",
    "        print(seq)\n",
    "        full_generation.append(pred)\n",
    "        predicted_strings.append(pred)\n",
    "        # print(\"PRED COMPUTED\")\n",
    "        # print(pred)\n",
    "        pred_label = translate_prediction_to_label(pred)\n",
    "        # print(\"PRED LABEL COMPUTED\")\n",
    "        # print(pred_label)\n",
    "        predictions_test.append(pred_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3dbb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:750: UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     local_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m world_size \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------Preparing the Data-----------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_________________________________\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:2330\u001b[0m, in \u001b[0;36mget_world_size\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[0;32m   2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1096\u001b[0m, in \u001b[0;36m_get_group_size\u001b[1;34m(group)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a given group's world size.\"\"\"\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mc:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributed\\distributed_c10d.py:1302\u001b[0m, in \u001b[0;36m_get_default_group\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[1;32m-> 1302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1305\u001b[0m     )\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember\u001b[38;5;241m.\u001b[39mWORLD)\n",
      "\u001b[1;31mValueError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "\n",
    "times_array = list(df[\"time\"].unique())\n",
    "datasets = []\n",
    "dataset_names = list(df[\"task\"].unique())\n",
    "\n",
    "for time in times_array:\n",
    "\n",
    "    time_ds = []\n",
    "    for split in df[\"split\"].unique():\n",
    "\n",
    "        split_df = df[(df[\"split\"] == split) & (df[\"time\"] == time)]\n",
    "        hf_split = Dataset.from_pandas(split_df)\n",
    "        time_ds.append(hf_split)\n",
    "    datasets.append(time_ds)\n",
    "\n",
    "hf_datasets = []\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "\n",
    "    hf_ds = DatasetDict({dataset[0][\"split\"][0]: dataset[0], \n",
    "                        dataset[1][\"split\"][0]: dataset[1],\n",
    "                        dataset[2][\"split\"][0]: dataset[2]})\n",
    "    hf_ds_name = dataset_names[i]\n",
    "    hf_datasets.append({hf_ds_name: hf_ds})\n",
    "\n",
    "hf_datasets = [\n",
    "    {task_name: hf_time.map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items() \n",
    "]\n",
    "\n",
    "n_samples_per_ds = [\n",
    "    len(hf_time[\"train\"])\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items() \n",
    "]\n",
    "\n",
    "for ds in hf_datasets:\n",
    "    for hf_data in ds.values():\n",
    "        hf_data.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \n",
    "                \"target\", \"implied_statement\", \"split\", \"time\", \"task\",\n",
    "                \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "hf_datasets = [\n",
    "    {task_name: {split: hf_time[split].remove_columns(cols_to_remove)}}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items()\n",
    "    for split in hf_time \n",
    "    if split != \"test\"]\n",
    "\n",
    "print(\"hf_datasets before data collator:\")\n",
    "print(hf_datasets)\n",
    "print()\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# distributed_samplers = [\n",
    "#     {task_name: {split: DistributedSampler(hf_time[split], num_replicas=world_size, rank=local_rank, shuffle=False)}}\n",
    "#     for hf_data in hf_datasets\n",
    "#     for task_name, hf_time in hf_data.items()\n",
    "#     for split in hf_time \n",
    "#     if split != \"test\"\n",
    "# ]\n",
    "\n",
    "distributed_samplers = []\n",
    "for ds in hf_datasets:\n",
    "    ds_dict = {}\n",
    "    print(\"ds:\")\n",
    "    print(ds)\n",
    "    for task_name, hf_data in ds.items():\n",
    "        print(\"task_name:\")\n",
    "        print(task_name)\n",
    "        print(\"hf_data:\")\n",
    "        print(hf_data)\n",
    "        ds_dict[task_name] = {}\n",
    "        for split in hf_data:\n",
    "            print(\"split:\")\n",
    "            print(split)\n",
    "            if split != \"test\":\n",
    "                distr_sampler = DistributedSampler(hf_data[split], num_replicas=world_size, rank=local_rank, shuffle=False)\n",
    "                ds_dict[task_name][split] = distr_sampler\n",
    "        dsitr_samplers.append(ds_dict)\n",
    "\n",
    "data_loaders = []\n",
    "for i, distr_sampler in enumerate(distributed_samplers):\n",
    "    ds_name = list(distr_sampler.keys())[0]\n",
    "    ds_dict = {}\n",
    "    ds_dict[ds_name] = {}\n",
    "    for split, distributed_sampler in distr_sampler[ds_name].items():\n",
    "        data_loader = DataLoader(hf_datasets[i][ds_name][split], collate_fn=data_collator, batch_size=batch_size, sampler=distributed_sampler)\n",
    "        ds_dict[ds_name][split] = data_loader\n",
    "    data_loaders.append(ds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd59cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "distributed_samplers = [\n",
    "    {task_name: {split: DistributedSampler(hf_time[split], num_replicas=world_size, rank=local_rank, shuffle=False)}}\n",
    "    for hf_data in hf_datasets\n",
    "    for task_name, hf_time in hf_data.items()\n",
    "    for split in hf_time \n",
    "    if split != \"test\"\n",
    "]\n",
    "\n",
    "data_loaders = []\n",
    "for i, distr_sampler in enumerate(distributed_samplers):\n",
    "    ds_name = list(distr_sampler.keys())[0]\n",
    "    ds_dict = {}\n",
    "    ds_dict[ds_name] = {}\n",
    "    for split, distributed_sampler in distr_sampler[ds_name].items():\n",
    "        data_loader = DataLoader(hf_datasets[i][ds_name][split], collate_fn=data_collator, batch_size=batch_size, sampler=distributed_sampler)\n",
    "        ds_dict[ds_name][split] = data_loader\n",
    "    data_loaders.append(ds_dict)\n",
    "\n",
    "# data loader = []\n",
    "# each item in the list is a dictionary of {<dataset_name>: {<split>: <dataloade>}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc206b0",
   "metadata": {},
   "source": [
    "## Model Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97446dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fe56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Models/Llama-3.2-1B-Instruct/Model\"\n",
    "tokenizer_id = \"Models/Llama-3.2-1B-Instruct/Tokenizer\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# model.save_pretrained(\"Models/Llama-3.2-1B-Instruct/Model\")\n",
    "# tokenizer.save_pretrained(\"Models/Llama-3.2-1B-Instruct/Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") \n",
    "# log_hf()\n",
    "load_dotenv(\"env_vars.env\")\n",
    "\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_epochs = 2\n",
    "lr = 1e-5\n",
    "lora_r = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546388f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Preapring the Data\n"
     ]
    }
   ],
   "source": [
    "########################################################## DATA WORK\n",
    "print(\"_________________________________\")\n",
    "print(\"Preapring the Data\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"df_from_exp_to_imp.csv\")\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09a2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66202d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abdf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = '<|finetune_right_pad_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d07937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|finetune_right_pad_id|>\n",
      "list[0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(type(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b884688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.pad_token, add_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab66f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafd5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "print(tokens.shape)\n",
    "second = tokens.fill_(-100)\n",
    "tok = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# torch_tensor = torch.tensor(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e14b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100, 128000,   9906,\n",
       "             11,   1268,    527,    499,     30]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both = torch.cat((tokens,tok), dim=1)\n",
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cd57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(both)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e711b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|finetune_right_pad_id|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=312):\n",
    "\n",
    "    # if type(label) != list:\n",
    "    #     label = [label]\n",
    "    # if type(clean_post) != list:\n",
    "    #     clean_post = [clean_post]\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    # pp(prompt_plus_messages)\n",
    "    # pp(label)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    # print(messages)\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    # print(chat_template)\n",
    "\n",
    "    # why is the chat template putting a new line at the end of the end of sequence\n",
    "    # pp(chat_template)\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "    # print(input_ids_tokenized)\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # acc to llama docs\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    print(\"FILLING PAD WITH\")\n",
    "    print(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    eos_n_tensor.fill_(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0])\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(logits, labels):\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "t_1 = []\n",
    "t_2 = []\n",
    "\n",
    "for split in df[\"split\"].unique():\n",
    "\n",
    "    split_df_1 = df[(df[\"split\"] == split) & (df[\"time\"] == 1)]\n",
    "    split_df_2 = df[(df[\"split\"] == split) & (df[\"time\"] == 2)]\n",
    "\n",
    "    hf_split_1 = Dataset.from_pandas(split_df_1)\n",
    "    hf_split_2 = Dataset.from_pandas(split_df_2)\n",
    "    \n",
    "    t_1.append(hf_split_1)\n",
    "    t_2.append(hf_split_2)\n",
    "\n",
    "hf_time_1 = DatasetDict({t_1[0][\"split\"][0]: t_1[0], \n",
    "                        t_1[1][\"split\"][0]: t_1[1],\n",
    "                        t_1[2][\"split\"][0]: t_1[2]})\n",
    "\n",
    "hf_time_2 = DatasetDict({t_2[0][\"split\"][0]: t_2[0], \n",
    "                        t_2[1][\"split\"][0]: t_2[1],\n",
    "                        t_2[2][\"split\"][0]: t_2[2]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = hf_time_1[\"train\"][0]\n",
    "input_model = preprocess_and_tokenize(ex[\"clean_post\"], ex[\"label\"], base_prompt=base_prompt, max_length=312)\n",
    "input_model[\"labels\"]\n",
    "input_model[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = model(**input_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd14464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3625, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c4d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 311, 128256])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a638b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 311])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_model[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01330643",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output_model.logits\n",
    "labels = input_model[\"labels\"]\n",
    "\n",
    "flat_logits = logits.view(-1, logits.size(-1))\n",
    "flat_labels = labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523653e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311, 128256])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eaaf9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af1681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9375, dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss(ignore_index=-100)  \n",
    "loss = loss_fn(flat_logits, flat_labels)          \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78dd8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([311])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7edf5",
   "metadata": {},
   "source": [
    "Checking that the -100 is not being computed!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32272c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (flat_labels != -100)\n",
    "mask.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b55fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_logits = flat_logits[mask]   \n",
    "valid_labels = flat_labels[mask]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12140e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.9375, dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_loss = loss_fn(valid_logits, valid_labels)\n",
    "manual_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(logits, labels):\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 1300.52 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:04<00:00, 1307.97 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 1364.92 examples/s]\n",
      "Map: 100%|██████████| 1019/1019 [00:00<00:00, 1268.26 examples/s]\n",
      "Map: 100%|██████████| 8155/8155 [00:06<00:00, 1311.17 examples/s]\n",
      "Map: 100%|██████████| 1020/1020 [00:00<00:00, 1156.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "########################################################## TOKENIZER WORK\n",
    "\n",
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "hf_time_2 = hf_time_2.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaec1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT HATEFUL'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_time_1[\"train\"][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79750cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(151643)\n",
    "end_of_text_token = 151643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_encoding = tokenizer.encode(\"HATEFUL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a221fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hate_encoding = tokenizer.encode(\"NOT HATEFUL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcef0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'formatted_prompt', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = hf_time_1[\"train\"][0]\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135abd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_prompt = example[\"input_ids\"][0].index(end_of_text_token)\n",
    "end_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b8e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14065, 472, 2336, 49636, 151643]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoding = tokenizer.encode(example[\"label\"] + tokenizer.eos_token)\n",
    "label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226d8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(14065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d17091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_answer = end_prompt-(len(label_encoding)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0][: start_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81880492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT HATEFUL<|im_end|>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0][start_answer : end_prompt - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'formatted_prompt', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01060ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdb98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0739894",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=312):\n",
    "\n",
    "    # if type(label) != list:\n",
    "    #     label = [label]\n",
    "    # if type(clean_post) != list:\n",
    "    #     clean_post = [clean_post]\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    # pp(prompt_plus_messages)\n",
    "    # pp(label)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    # print(messages)\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    # print(chat_template)\n",
    "\n",
    "    # why is the chat template putting a new line at the end of the end of sequence\n",
    "    # pp(chat_template)\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "    # print(input_ids_tokenized)\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # acc to llama docs\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.eos_token_id)\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171650b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 821.43 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:07<00:00, 818.96 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:01<00:00, 622.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_time_1.set_format(\"torch\")\n",
    "hf_time_2.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \"target\", \"implied_statement\", \"split\", \"time\", \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "for split in hf_time_1:\n",
    "    if split != \"test\":\n",
    "        hf_time_1[split] = hf_time_1[split].remove_columns(cols_to_remove)\n",
    "        hf_time_2[split] = hf_time_2[split].remove_columns(cols_to_remove)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "hf_time_1_train_loader = DataLoader(hf_time_1[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_validation_loader = DataLoader(hf_time_1[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_test_loader = DataLoader(hf_time_1[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "hf_time_2_train_loader = DataLoader(hf_time_2[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_validation_loader = DataLoader(hf_time_2[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_test_loader = DataLoader(hf_time_2[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "# ### So far, created the prompt, did the messages with the prompt and answer in place. Applied to chat template and tokenized \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Loading the model and model config\n",
      "Model Size before LoRA 315119488\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "\n",
      "Model After LoRA\n",
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "########################3#################### MODEL WORK\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading the model and model config\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\",\n",
    "                                            quantization_config=bnb_config\n",
    "                                            )\n",
    "\n",
    "# to deal with the fact that we dont make the first token prediction??\n",
    "\n",
    "\n",
    "model_size_before = sum(t.numel() for t in model.parameters())\n",
    "print(\"Model Size before LoRA\", model_size_before)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print(\"Model After LoRA\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Training the model\")\n",
    "print()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.train()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(hf_time_1_train_loader):\n",
    "        if i > 0:\n",
    "            continue\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"\\tBatch: \", i)\n",
    "        # print(batch)\n",
    "        batch.to(device)\n",
    "        # print(batch.keys())\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        output = model(**batch)\n",
    "        logits = output.logits\n",
    "        loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "        print(batch.keys())\n",
    "        print(loss.detach().item())\n",
    "        print(output.logits.shape)\n",
    "        print(output.probas)\n",
    "\n",
    "        if i > 3:\n",
    "            continue\n",
    "\n",
    "    epoch_loss = sum(losses)/len(hf_time_1_train_loader)\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        for i, batch in enumerate(hf_time_1_validation_loader):\n",
    "            if i > 0:\n",
    "                continue\n",
    "            batch.to(device)\n",
    "            batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "            logits = output.logits\n",
    "            val_loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "            val_losses.append(val_loss.detach().item())\n",
    "\n",
    "        val_loss_epoch = sum(val_losses)/len(hf_time_1_validation_loader)\n",
    "        print(f\"Epoch {epoch} Validation Loss: {val_loss_epoch}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "for i, test_batch in enumerate(hf_time_1[\"test\"]):\n",
    "\n",
    "    if i > 0:\n",
    "        break\n",
    "    \n",
    "    text = test_batch[\"formatted_prompt\"]\n",
    "    tokenized_chat_template, messages_list = preprocess_and_tokenize(text, label=False, add_generation_prompt=True, output_messages_list=True)\n",
    "    output = model.generate(**tokenized_chat_template.to(device))\n",
    "    pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(text)\n",
    "    print(tokenized_chat_template)\n",
    "    print(output)\n",
    "    print(pred)\n",
    "\n",
    "print(\"CHECKING GENERATION\")\n",
    "print(messages_list)\n",
    "\n",
    "print(tokenized_chat_template)\n",
    "print(output)\n",
    "\n",
    "print(type(output))\n",
    "print(output.shape)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Saving the model and Tokenizer\")\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "model.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "tokenizer.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "\n",
    "print(\"RUN SUCCESSFULLY\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

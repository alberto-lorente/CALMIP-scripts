{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1e57cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import sys \n",
    "import warnings \n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import whoami, HfFolder\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549fe56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Models/Llama-3.2-1B-Instruct/Model\"\n",
    "tokenizer_id = \"Models/Llama-3.2-1B-Instruct/Tokenizer\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "\n",
    "# model.save_pretrained(\"Models/Llama-3.2-1B-Instruct/Model\")\n",
    "# tokenizer.save_pretrained(\"Models/Llama-3.2-1B-Instruct/Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb35d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") \n",
    "# log_hf()\n",
    "load_dotenv(\"env_vars.env\")\n",
    "\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41adceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_epochs = 2\n",
    "lr = 1e-5\n",
    "lora_r = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "546388f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Preapring the Data\n"
     ]
    }
   ],
   "source": [
    "########################################################## DATA WORK\n",
    "print(\"_________________________________\")\n",
    "print(\"Preapring the Data\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"df_from_exp_to_imp.csv\")\n",
    "\n",
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be09a2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66202d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49b3cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44abdf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = '<|finetune_right_pad_id|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42d07937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|finetune_right_pad_id|>\n",
      "list[0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(type(tokenizer.encode(tokenizer.pad_token, add_special_tokens=False))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b884688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.pad_token, add_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ab66f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128004"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(tokenizer.pad_token, add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfafd5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "print(tokens.shape)\n",
    "second = tokens.fill_(-100)\n",
    "tok = tokenizer.encode(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# torch_tensor = torch.tensor(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "29e14b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100, 128000,   9906,\n",
       "             11,   1268,    527,    499,     30]])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both = torch.cat((tokens,tok), dim=1)\n",
    "both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2f9cd57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "517b1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = model(both)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4b6e711b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dfc8d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "#### Attaching the prompt to the clean post\n",
    "\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "t_1 = []\n",
    "t_2 = []\n",
    "\n",
    "for split in df[\"split\"].unique():\n",
    "\n",
    "    split_df_1 = df[(df[\"split\"] == split) & (df[\"time\"] == 1)]\n",
    "    split_df_2 = df[(df[\"split\"] == split) & (df[\"time\"] == 2)]\n",
    "\n",
    "    hf_split_1 = Dataset.from_pandas(split_df_1)\n",
    "    hf_split_2 = Dataset.from_pandas(split_df_2)\n",
    "    \n",
    "    t_1.append(hf_split_1)\n",
    "    t_2.append(hf_split_2)\n",
    "\n",
    "hf_time_1 = DatasetDict({t_1[0][\"split\"][0]: t_1[0], \n",
    "                        t_1[1][\"split\"][0]: t_1[1],\n",
    "                        t_1[2][\"split\"][0]: t_1[2]})\n",
    "\n",
    "hf_time_2 = DatasetDict({t_2[0][\"split\"][0]: t_2[0], \n",
    "                        t_2[1][\"split\"][0]: t_2[1],\n",
    "                        t_2[2][\"split\"][0]: t_2[2]})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d238e5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 1300.52 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:04<00:00, 1307.97 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:00<00:00, 1364.92 examples/s]\n",
      "Map: 100%|██████████| 1019/1019 [00:00<00:00, 1268.26 examples/s]\n",
      "Map: 100%|██████████| 8155/8155 [00:06<00:00, 1311.17 examples/s]\n",
      "Map: 100%|██████████| 1020/1020 [00:00<00:00, 1156.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "########################################################## TOKENIZER WORK\n",
    "\n",
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "hf_time_2 = hf_time_2.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebaec1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT HATEFUL'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_time_1[\"train\"][0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79750cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67cb97a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151643"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7482fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(151643)\n",
    "end_of_text_token = 151643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5891c447",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_encoding = tokenizer.encode(\"HATEFUL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66a221fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_hate_encoding = tokenizer.encode(\"NOT HATEFUL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddcef0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'formatted_prompt', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = hf_time_1[\"train\"][0]\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "135abd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_prompt = example[\"input_ids\"][0].index(end_of_text_token)\n",
    "end_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24b8e2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14065, 472, 2336, 49636, 151643]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoding = tokenizer.encode(example[\"label\"] + tokenizer.eos_token)\n",
    "label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6226d8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(14065)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d17091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_answer = end_prompt-(len(label_encoding)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0][: start_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81880492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOT HATEFUL<|im_end|>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example[\"input_ids\"][0][start_answer : end_prompt - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7a9e8d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clean_post', 'post', 'class', 'implicit_class', 'extra_implicit_class', 'target', 'implied_statement', 'split', 'time', 'formatted_prompt', 'label', '__index_level_0__', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01060ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "10fdb98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0739894",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None: tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "\n",
    "def preprocess_and_tokenize(clean_post, label, base_prompt=base_prompt, max_length=312):\n",
    "\n",
    "    # if type(label) != list:\n",
    "    #     label = [label]\n",
    "    # if type(clean_post) != list:\n",
    "    #     clean_post = [clean_post]\n",
    "    \n",
    "    prompt_plus_messages = base_prompt.format(clean_post)\n",
    "    # pp(prompt_plus_messages)\n",
    "    # pp(label)\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_plus_messages},\n",
    "            {\"role\": \"assistant\", \"content\": label.strip(\"\\n\")}\n",
    "        ]\n",
    "\n",
    "    # print(messages)\n",
    "    chat_template = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=False, add_special_tokens=False).rstrip()\n",
    "    # print(chat_template)\n",
    "\n",
    "    # why is the chat template putting a new line at the end of the end of sequence\n",
    "    # pp(chat_template)\n",
    "    input_ids_tokenized = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    # getting the normal text just to know how much we need to add to the left as -100 and right as pad token\n",
    "    input_ids_shape = tokenizer(chat_template, return_tensors=\"pt\", add_special_tokens=False, padding=False)[\"input_ids\"]\n",
    "    # print(input_ids_tokenized)\n",
    "\n",
    "    # getting the label target to only predict the actual label and ignore the prompt\n",
    "    labels_tokenized = tokenizer(label + tokenizer.eos_token, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    shape = input_ids_shape.shape[1] - labels_tokenized.shape[1]\n",
    "    zeros = torch.zeros((1, shape), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    zeros.fill_(-100) # acc to llama docs\n",
    "    labels_left_padded = torch.cat([zeros, labels_tokenized], dim=1)\n",
    "\n",
    "    eos_n = input_ids_tokenized.shape[1] - labels_left_padded.shape[1]\n",
    "    eos_n_tensor = torch.zeros((1, eos_n), dtype=labels_tokenized.dtype, device=labels_tokenized.device)\n",
    "    eos_n_tensor.fill_(tokenizer.eos_token_id)\n",
    "    labels_padded = torch.cat([labels_left_padded, eos_n_tensor], dim=1)\n",
    "\n",
    "    # print(labels_padded.shape == input_ids_tokenized.shape)\n",
    "\n",
    "    # shifting because we dont predict the first token\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_padded[:, 1:]\n",
    "\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "171650b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 718/718 [00:00<00:00, 821.43 examples/s]\n",
      "Map: 100%|██████████| 5752/5752 [00:07<00:00, 818.96 examples/s]\n",
      "Map: 100%|██████████| 720/720 [00:01<00:00, 622.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"clean_post\", \"label\"], batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_time_1.set_format(\"torch\")\n",
    "hf_time_2.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \"target\", \"implied_statement\", \"split\", \"time\", \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "for split in hf_time_1:\n",
    "    if split != \"test\":\n",
    "        hf_time_1[split] = hf_time_1[split].remove_columns(cols_to_remove)\n",
    "        hf_time_2[split] = hf_time_2[split].remove_columns(cols_to_remove)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "\n",
    "hf_time_1_train_loader = DataLoader(hf_time_1[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_validation_loader = DataLoader(hf_time_1[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_test_loader = DataLoader(hf_time_1[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "hf_time_2_train_loader = DataLoader(hf_time_2[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_validation_loader = DataLoader(hf_time_2[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_test_loader = DataLoader(hf_time_2[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "# ### So far, created the prompt, did the messages with the prompt and answer in place. Applied to chat template and tokenized \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c0c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Loading the model and model config\n",
      "Model Size before LoRA 315119488\n",
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "\n",
      "Model After LoRA\n",
      "trainable params: 1,081,344 || all params: 495,114,112 || trainable%: 0.2184\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "########################3#################### MODEL WORK\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading the model and model config\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\",\n",
    "                                            quantization_config=bnb_config\n",
    "                                            )\n",
    "\n",
    "# to deal with the fact that we dont make the first token prediction??\n",
    "\n",
    "\n",
    "model_size_before = sum(t.numel() for t in model.parameters())\n",
    "print(\"Model Size before LoRA\", model_size_before)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "lora_alpha = lora_r*2\n",
    "config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print(\"Model After LoRA\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Training the model\")\n",
    "print()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.train()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(hf_time_1_train_loader):\n",
    "        if i > 0:\n",
    "            continue\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"\\tBatch: \", i)\n",
    "        # print(batch)\n",
    "        batch.to(device)\n",
    "        # print(batch.keys())\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        output = model(**batch)\n",
    "        logits = output.logits\n",
    "        loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "        print(batch.keys())\n",
    "        print(loss.detach().item())\n",
    "        print(output.logits.shape)\n",
    "        print(output.probas)\n",
    "\n",
    "        if i > 3:\n",
    "            continue\n",
    "\n",
    "    epoch_loss = sum(losses)/len(hf_time_1_train_loader)\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        for i, batch in enumerate(hf_time_1_validation_loader):\n",
    "            if i > 0:\n",
    "                continue\n",
    "            batch.to(device)\n",
    "            batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "            logits = output.logits\n",
    "            val_loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "            val_losses.append(val_loss.detach().item())\n",
    "\n",
    "        val_loss_epoch = sum(val_losses)/len(hf_time_1_validation_loader)\n",
    "        print(f\"Epoch {epoch} Validation Loss: {val_loss_epoch}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d6c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "for i, test_batch in enumerate(hf_time_1[\"test\"]):\n",
    "\n",
    "    if i > 0:\n",
    "        break\n",
    "    \n",
    "    text = test_batch[\"formatted_prompt\"]\n",
    "    tokenized_chat_template, messages_list = preprocess_and_tokenize(text, label=False, add_generation_prompt=True, output_messages_list=True)\n",
    "    output = model.generate(**tokenized_chat_template.to(device))\n",
    "    pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(text)\n",
    "    print(tokenized_chat_template)\n",
    "    print(output)\n",
    "    print(pred)\n",
    "\n",
    "print(\"CHECKING GENERATION\")\n",
    "print(messages_list)\n",
    "\n",
    "print(tokenized_chat_template)\n",
    "print(output)\n",
    "\n",
    "print(type(output))\n",
    "print(output.shape)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Saving the model and Tokenizer\")\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "model.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "tokenizer.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "\n",
    "print(\"RUN SUCCESSFULLY\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

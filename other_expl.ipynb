{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7320123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import os \n",
    "import sys \n",
    "import warnings \n",
    "import random\n",
    "from pprint import pprint as pp\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import whoami, HfFolder\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "from transformers import set_seed, Seq2SeqTrainer, LlamaTokenizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d5052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"You are a social media content moderator.\n",
    "INSTRUCTION: The following is a social media message that needs to be classified with the label HATEFUL or NOT HATEFUL.\n",
    "MESSAGE: {}\n",
    "OUTPUT AND FORMAT: your output should be just the label.\"\"\"\n",
    "\n",
    "\n",
    "def log_hf():\n",
    "    \n",
    "    load_dotenv(\"env_vars.env\")\n",
    "    hf_token = os.environ.get(\"HF_ACCESS_TOKEN\")\n",
    "    HfFolder.save_token(hf_token)\n",
    "    return print(whoami()[\"name\"])\n",
    "\n",
    "def format_prompt(text, base_prompt=base_prompt):\n",
    "\n",
    "    formatted_prompt = base_prompt.format(text)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "def translate_class_to_label(class_):\n",
    "\n",
    "    translation_dict = {\"not_hate\": \"NOT HATEFUL\",\n",
    "                        \"explicit_hate\": \"HATEFUL\",\n",
    "                        \"implicit_hate\": \"HATEFUL\"}\n",
    "\n",
    "    translated_label = translation_dict[class_]\n",
    "\n",
    "    return translated_label\n",
    "\n",
    "def format_message(formatted_prompt, label=True):\n",
    "    if label:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt},\n",
    "            {\"role\": \"assistant\", \"content\": label}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ]\n",
    "    return messages\n",
    "\n",
    "# def keytoken_weighted_loss(inputs, logits):\n",
    "#     # Shift so that tokens < n predict n\n",
    "#     shift_labels = inputs[..., 1:].contiguous()\n",
    "#     shift_logits = logits[..., :-1, :].contiguous()\n",
    "#     # Calculate per-token loss\n",
    "#     loss_fct = CrossEntropyLoss(reduce=False)\n",
    "#     loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "#     # Resize and average loss per sample\n",
    "#     loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "\n",
    "#     return loss_per_sample\n",
    "\n",
    "\n",
    "# Do I need to apply the chat template????????????????\n",
    "\n",
    "# padding (`bool`, defaults to `False`):\n",
    "#     Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`.\n",
    "# truncation (`bool`, defaults to `False`):\n",
    "#     Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n",
    "# max_length (`int`, *optional*):\n",
    "#     Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n",
    "#     not specified, the tokenizer's `max_length` attribute will be used as a default.\n",
    "\n",
    "\n",
    "def preprocess_and_tokenize(formatted_prompt, label=True, add_generation_prompt=False, context_length=512, output_messages_list=False):\n",
    "\n",
    "    messages = format_message(formatted_prompt, label)\n",
    "    tokenized = tokenizer.apply_chat_template(messages, \n",
    "                                                tokenize=True, \n",
    "                                                add_generation_prompt=add_generation_prompt,\n",
    "                                                padding=\"max_length\",\n",
    "                                                truncation=True,\n",
    "                                                max_length=context_length,\n",
    "                                                return_dict=True,\n",
    "                                                return_tensors=\"pt\")\n",
    "    if output_messages_list:\n",
    "        return tokenized, messages\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d9dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") \n",
    "# log_hf()\n",
    "load_dotenv(\"env_vars.env\")\n",
    "\n",
    "set_seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4e43da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________\n",
      "Preapring the Data\n"
     ]
    }
   ],
   "source": [
    "########################################################## DATA WORK\n",
    "print(\"_________________________________\")\n",
    "print(\"Preapring the Data\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"df_from_exp_to_imp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2802f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_targets = list(df[\"target\"].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e88ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_targets = list(set([target.lower() for target in list_targets])) # removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22d6b48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43825ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"white liberals\" in list_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d37dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Anti-LGBTQ organizations, Degenerates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edce648",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_dictionary = {\"immigrants\":     [\"Immigrants.\", \n",
    "                                            \"immigrants\", \n",
    "                                            \"Mexican immigratns\",\n",
    "                                            \"mexicans and mestizos\",\n",
    "                                            \"foreigners\",\n",
    "                                            \"muslim immigrants\",\n",
    "                                            \"irish and polish immigrants\",\n",
    "                                            \"muslim refugees\",\n",
    "                                            \"latino immigrants\",\n",
    "                                            \"illegal aliens\",\n",
    "                                            \"aliens\",\n",
    "                                            \"outsiders\"],\n",
    "\n",
    "                                \"political\":      [\"Progressives\", \n",
    "                                                \"Democrats\", \n",
    "                                                \"Republicans\", \n",
    "                                                \"Liberals\", \n",
    "                                                \"Right wing conservatives\", \n",
    "                                                \"Conservatives\", \n",
    "                                                \"white liberals\", \n",
    "                                                \"white liberal women\", \n",
    "                                                \"Progressive Indians\",\n",
    "                                                \"religious conservatives\",\n",
    "                                                \"alt-right people\",\n",
    "                                                \"white progressives\",\n",
    "                                                \"muslims, liberals, protesters\",\n",
    "                                                \"black activists\",\n",
    "                                                \"anti-gun activist.\",\n",
    "                                                \"conservatives\",\n",
    "                                                \"white conservatives\",\n",
    "                                                \"conservative males\",\n",
    "                                                \"left-wing liberals\",\n",
    "                                                \"antifa\",\n",
    "                                                \"politician\",\n",
    "                                                \"male conservatives\",\n",
    "                                                \"democratics\",\n",
    "                                                \"nationalists\",\n",
    "                                                \"lefties\",\n",
    "                                                \"capitalists\",\n",
    "                                                \"trump supporters\",\n",
    "                                                \"liberals\",\n",
    "                                                \"republicans\",\n",
    "                                                \"leftists\"],\n",
    "\n",
    "                                \"religious\":        [\"Jews\", \n",
    "                                                \"jewish\",\n",
    "                                                \"Muslim people\", \n",
    "                                                \"Christians\", \n",
    "                                                \"Jewish People\", \n",
    "                                                \"Hinduists\", \n",
    "                                                \"Atheists\", \n",
    "                                                \"White people, christians\", \n",
    "                                                \"muslims\", \n",
    "                                                \"Islam people.\",\n",
    "                                                \"religious people.\",\n",
    "                                                \"christian leaders\",\n",
    "                                                \"non-christian whites\",\n",
    "                                                \"religious people\",\n",
    "                                                \"muslims, liberals, protesters\",\n",
    "                                                \"christians, jew\",\n",
    "                                                \"muslim children\",\n",
    "                                                \"muslim immigrants\",\n",
    "                                                \"islamic followers\",\n",
    "                                                \"non-christians\",\n",
    "                                                \"pakistani,iranian,islamic,jewish\",\n",
    "                                                \"white christians\",\n",
    "                                                \"catholics\",\n",
    "                                                \"atheists\",\n",
    "                                                \"jews and arabs\"],\n",
    "\n",
    "                                \"lbtq+\":        [\"homosexuals\",\n",
    "                                                \"Gay folks\",\n",
    "                                                \"transgeners\",\n",
    "                                                \"lgbt people, specially transsexuals.\",\n",
    "                                                \"gays\",\n",
    "                                                \"lgbqti\",\n",
    "                                                \"transexual\",\n",
    "                                                \"gay people\"],\n",
    "                                                \n",
    "                                \"xenophobic\":       [\"muslim and black folks\",\n",
    "                                                \"black people\",\n",
    "                                                \"immigrants and other minority groups.\",\n",
    "                                                \"mexicans and mestizos\",\n",
    "                                                \"pakistanis\",\n",
    "                                                \"minorities, mainly black people.\",\n",
    "                                                \"italians\",\n",
    "                                                \"muslim children\",\n",
    "                                                \"foreigners\",\n",
    "                                                \"irish and polish immigrants\",\n",
    "                                                \"black and asian people.\",\n",
    "                                                \"indian folks\",\n",
    "                                                \"islamic followers\",\n",
    "                                                \"pakistani,iranian,islamic,jewish\",\n",
    "                                                \"hispanics\",\n",
    "                                                \"indian women\",\n",
    "                                                \"jews and arabs\"\n",
    "                                                ],\n",
    "                                        \n",
    "                                \"other\": [\"not specified\",\n",
    "                                                \"poor whites\",\n",
    "                                                \"robert\",\n",
    "                                                \"neo nazis and white supremacist\",\n",
    "                                                \"white america\",\n",
    "                                                \"rich people\",\n",
    "                                                \"furries\",\n",
    "                                                \"racists\",\n",
    "                                                \"impossible to determine without more information or context\"],\n",
    "\n",
    "                                \"race\":         [\"white people\",\n",
    "                                                \"middle east people\",\n",
    "                                                \"black activists\",\n",
    "                                                \"mexicans and mestizos\",\n",
    "                                                \"pakistanis\",\n",
    "                                                \"minorities, mainly black people.\",\n",
    "                                                \"color people\",\n",
    "                                                \"black and asian people.\",\n",
    "                                                \"indian folks\",\n",
    "                                                \"pakistani,iranian,islamic,jewish\",\n",
    "                                                \"hispanics\",\n",
    "                                                \"indian women\",\n",
    "                                                \"mixed race\"\n",
    "                                                ],\n",
    "\n",
    "                                \"misogyny\": [\"womens\",\n",
    "                                                \"fat white women\",\n",
    "                                                \"indian women\",\n",
    "\n",
    "                                    ]\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1bdbbe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['immigrants', 'political', 'religious', 'lbtq+', 'xenophobic', 'other', 'race', 'misogyny'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_dictionary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f0297af",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = \"\"\n",
    "list_done = [v for l in translation_dictionary.values() for v in l]\n",
    "for target in list_targets:\n",
    "    if target not in list_done:\n",
    "        str_ = str_ + target + \"\\n\"\n",
    "with open(\"list_targets.txt\", \"w\") as f:\n",
    "    f.write(str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "63ec43b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ba27024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts().values > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd0ee4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Minorities', 'Immigrants', 'white people', 'Muslims', 'Jews', 'Blacks',\n",
       "       'non-white people'],\n",
       "      dtype='object', name='target')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts().index[df[\"target\"].value_counts().values > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60af50b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_post</th>\n",
       "      <th>post</th>\n",
       "      <th>class</th>\n",
       "      <th>implicit_class</th>\n",
       "      <th>extra_implicit_class</th>\n",
       "      <th>target</th>\n",
       "      <th>implied_statement</th>\n",
       "      <th>split</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>university of washington ( seattle  wa ) #proj...</td>\n",
       "      <td>university of washington ( seattle  wa ) #proj...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>validation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no excuse not to - neoliberal media / establis...</td>\n",
       "      <td>: no excuse not to - neoliberal media / establ...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human being is a white termanimals ... are wha...</td>\n",
       "      <td>human being is a white termanimals ... are wha...</td>\n",
       "      <td>explicit_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youre aware that australians are white  right ...</td>\n",
       "      <td>youre aware that australians are white  right ...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and those who are too stupid to realize or car...</td>\n",
       "      <td>and those who are too stupid to realize or car...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17379</th>\n",
       "      <td>blog : 3 reasons hawaiis law forcing pregnancy...</td>\n",
       "      <td>blog : 3 reasons hawaiis law forcing pregnancy...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17380</th>\n",
       "      <td>i see nothing wrong with pride in ones white r...</td>\n",
       "      <td>i see nothing wrong with pride in ones white r...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17381</th>\n",
       "      <td>american conservatism managed to kill both bod...</td>\n",
       "      <td>american conservatism managed to kill both bod...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>validation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17382</th>\n",
       "      <td>last time i looked  100 percent of homicide ar...</td>\n",
       "      <td>last time i looked  100 percent of homicide ar...</td>\n",
       "      <td>not_hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17383</th>\n",
       "      <td>what you fail to understand is whites are a gl...</td>\n",
       "      <td>what you fail to understand is whites are a gl...</td>\n",
       "      <td>implicit_hate</td>\n",
       "      <td>white_grievance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Immigrants</td>\n",
       "      <td>Immigrants are invaders</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17384 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_post  \\\n",
       "0      university of washington ( seattle  wa ) #proj...   \n",
       "1      no excuse not to - neoliberal media / establis...   \n",
       "2      human being is a white termanimals ... are wha...   \n",
       "3      youre aware that australians are white  right ...   \n",
       "4      and those who are too stupid to realize or car...   \n",
       "...                                                  ...   \n",
       "17379  blog : 3 reasons hawaiis law forcing pregnancy...   \n",
       "17380  i see nothing wrong with pride in ones white r...   \n",
       "17381  american conservatism managed to kill both bod...   \n",
       "17382  last time i looked  100 percent of homicide ar...   \n",
       "17383  what you fail to understand is whites are a gl...   \n",
       "\n",
       "                                                    post          class  \\\n",
       "0      university of washington ( seattle  wa ) #proj...       not_hate   \n",
       "1      : no excuse not to - neoliberal media / establ...       not_hate   \n",
       "2      human being is a white termanimals ... are wha...  explicit_hate   \n",
       "3      youre aware that australians are white  right ...       not_hate   \n",
       "4      and those who are too stupid to realize or car...       not_hate   \n",
       "...                                                  ...            ...   \n",
       "17379  blog : 3 reasons hawaiis law forcing pregnancy...       not_hate   \n",
       "17380  i see nothing wrong with pride in ones white r...       not_hate   \n",
       "17381  american conservatism managed to kill both bod...       not_hate   \n",
       "17382  last time i looked  100 percent of homicide ar...       not_hate   \n",
       "17383  what you fail to understand is whites are a gl...  implicit_hate   \n",
       "\n",
       "        implicit_class extra_implicit_class      target  \\\n",
       "0                  NaN                  NaN         NaN   \n",
       "1                  NaN                  NaN         NaN   \n",
       "2                  NaN                  NaN         NaN   \n",
       "3                  NaN                  NaN         NaN   \n",
       "4                  NaN                  NaN         NaN   \n",
       "...                ...                  ...         ...   \n",
       "17379              NaN                  NaN         NaN   \n",
       "17380              NaN                  NaN         NaN   \n",
       "17381              NaN                  NaN         NaN   \n",
       "17382              NaN                  NaN         NaN   \n",
       "17383  white_grievance                  NaN  Immigrants   \n",
       "\n",
       "             implied_statement       split  time  \n",
       "0                          NaN  validation     2  \n",
       "1                          NaN       train     2  \n",
       "2                          NaN  validation     1  \n",
       "3                          NaN       train     2  \n",
       "4                          NaN       train     1  \n",
       "...                        ...         ...   ...  \n",
       "17379                      NaN       train     2  \n",
       "17380                      NaN       train     2  \n",
       "17381                      NaN  validation     1  \n",
       "17382                      NaN       train     1  \n",
       "17383  Immigrants are invaders       train     2  \n",
       "\n",
       "[17384 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c282f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Attaching the prompt to the clean post\n",
    "\n",
    "df[\"formatted_prompt\"] = df[\"clean_post\"].apply(format_prompt)\n",
    "df[\"label\"] = df[\"class\"].apply(translate_class_to_label)\n",
    "\n",
    "# ### Turning the Df into a DatasetDict\n",
    "\n",
    "t_1 = []\n",
    "t_2 = []\n",
    "\n",
    "for split in df[\"split\"].unique():\n",
    "\n",
    "    split_df_1 = df[(df[\"split\"] == split) & (df[\"time\"] == 1)]\n",
    "    split_df_2 = df[(df[\"split\"] == split) & (df[\"time\"] == 2)]\n",
    "\n",
    "    hf_split_1 = Dataset.from_pandas(split_df_1)\n",
    "    hf_split_2 = Dataset.from_pandas(split_df_2)\n",
    "    \n",
    "    t_1.append(hf_split_1)\n",
    "    t_2.append(hf_split_2)\n",
    "\n",
    "hf_time_1 = DatasetDict({t_1[0][\"split\"][0]: t_1[0], \n",
    "                        t_1[1][\"split\"][0]: t_1[1],\n",
    "                        t_1[2][\"split\"][0]: t_1[2]})\n",
    "\n",
    "hf_time_2 = DatasetDict({t_2[0][\"split\"][0]: t_2[0], \n",
    "                        t_2[1][\"split\"][0]: t_2[1],\n",
    "                        t_2[2][\"split\"][0]: t_2[2]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################## TOKENIZER WORK\n",
    "\n",
    "model_id = \"Models/TinyLlama\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b98680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hf_time_1 = hf_time_1.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "hf_time_2 = hf_time_2.map(preprocess_and_tokenize, input_columns=[\"formatted_prompt\", \"label\"], batched=False)\n",
    "\n",
    "hf_time_1.set_format(\"torch\")\n",
    "hf_time_2.set_format(\"torch\")\n",
    "\n",
    "cols_to_remove = [\"clean_post\", \"post\", \"class\", \"implicit_class\", \"extra_implicit_class\", \"target\", \"implied_statement\", \"split\", \"time\", \"formatted_prompt\", \"label\", \"__index_level_0__\"]\n",
    "\n",
    "for split in hf_time_1:\n",
    "    if split != \"test\":\n",
    "        hf_time_1[split] = hf_time_1[split].remove_columns(cols_to_remove)\n",
    "        hf_time_2[split] = hf_time_2[split].remove_columns(cols_to_remove)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "hf_time_1_train_loader = DataLoader(hf_time_1[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_validation_loader = DataLoader(hf_time_1[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_1_test_loader = DataLoader(hf_time_1[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "hf_time_2_train_loader = DataLoader(hf_time_2[\"train\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_validation_loader = DataLoader(hf_time_2[\"validation\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "hf_time_2_test_loader = DataLoader(hf_time_2[\"test\"], collate_fn=data_collator, batch_size=batch_size)\n",
    "\n",
    "# ### So far, created the prompt, did the messages with the prompt and answer in place. Applied to chat template and tokenized \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################3#################### MODEL WORK\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Loading the model and model config\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "                                load_in_4bit= True,\n",
    "                                bnb_4bit_quant_type= \"nf4\",\n",
    "                                bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "                                bnb_4bit_use_double_quant= True,\n",
    "                            )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\",\n",
    "                                            quantization_config=bnb_config\n",
    "                                            )\n",
    "\n",
    "# to deal with the fact that we dont make the first token prediction??\n",
    "\n",
    "\n",
    "model_size_before = sum(t.numel() for t in model.parameters())\n",
    "print(\"Model Size before LoRA\", model_size_before)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print(\"Model After LoRA\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "lr = 1e-5\n",
    "optimizer = AdamW((param for param in model.parameters() if param.requires_grad), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d404ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## TRAINING AND TESTING\n",
    "n_epochs = 2\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Training the model\")\n",
    "print()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.train()\n",
    "\n",
    "    print(\"Epoch: \", epoch)\n",
    "    losses = []\n",
    "\n",
    "    for i, batch in enumerate(hf_time_1_train_loader):\n",
    "        if i > 0:\n",
    "            continue\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"\\tBatch: \", i)\n",
    "        # print(batch)\n",
    "        batch.to(device)\n",
    "        # print(batch.keys())\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "        # print(batch[\"input_ids\"].shape)\n",
    "        # print(batch[\"attention_mask\"].shape)\n",
    "        # print(batch[\"labels\"].shape)\n",
    "\n",
    "\n",
    "        output = model(**batch)\n",
    "        logits = output.logits\n",
    "        loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(loss.detach().item())\n",
    "\n",
    "        print(batch.keys())\n",
    "        print(loss.detach().item())\n",
    "        print(output.logits.shape)\n",
    "        print(output.probas)\n",
    "\n",
    "        if i > 3:\n",
    "            continue\n",
    "\n",
    "    epoch_loss = sum(losses)/len(hf_time_1_train_loader)\n",
    "    print(f\"Epoch {epoch} Loss: {epoch_loss}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        for i, batch in enumerate(hf_time_1_validation_loader):\n",
    "            if i > 0:\n",
    "                continue\n",
    "            batch.to(device)\n",
    "            batch = {k:torch.squeeze(v) for k,v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "            logits = output.logits\n",
    "            val_loss = loss_fn(logits, batch[\"labels\"])\n",
    "\n",
    "            val_losses.append(val_loss.detach().item())\n",
    "\n",
    "        val_loss_epoch = sum(val_losses)/len(hf_time_1_validation_loader)\n",
    "        print(f\"Epoch {epoch} Validation Loss: {val_loss_epoch}\")\n",
    "\n",
    "print()\n",
    "print(\"_________________________________\")\n",
    "print(\"Testing the model\")\n",
    "for i, test_batch in enumerate(hf_time_1[\"test\"]):\n",
    "\n",
    "    if i > 0:\n",
    "        break\n",
    "    \n",
    "    text = test_batch[\"formatted_prompt\"]\n",
    "    tokenized_chat_template, messages_list = preprocess_and_tokenize(text, label=False, add_generation_prompt=True, output_messages_list=True)\n",
    "    output = model.generate(**tokenized_chat_template.to(device))\n",
    "    pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(text)\n",
    "    print(tokenized_chat_template)\n",
    "    print(output)\n",
    "    print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d88578",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CHECKING GENERATION\")\n",
    "print(messages_list)\n",
    "\n",
    "print(tokenized_chat_template)\n",
    "print(output)\n",
    "\n",
    "print(type(output))\n",
    "print(output.shape)\n",
    "\n",
    "print(\"_________________________________\")\n",
    "print(\"Saving the model and Tokenizer\")\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "model.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "tokenizer.save_pretrained(f\"alberto-lorente/{model_name}_test\")\n",
    "\n",
    "print(\"RUN SUCCESSFULLY\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
